# AI 程序的抽象

大家好，上次课大家已经了解了组织中的两个 AI 编译器项目，今天继续有我带大家稍微深入细节，了解一下 AI 程序是如何在 AI 编译器中表示出来的。

要深入今天的话题，需要首先理解 AI 程序和一般的程序究竟有什么区别？换句话说，为什么我们不会像写一般的程序一样，使用一种图灵机等价的高级语言来实现一个一般的程序，用这个程序进行深度学习计算？

即使仅通过参加训练营之后的经验，大家应该也可以发现，用高级语言直接实现深度学习程序无疑本身是可行的，大家已经见到的，无论是 C++ 习题中的几个算法，还是专业阶段 AI 编译器或者大模型方向的习题，以及 InfiniLM、Llama.cpp 这样的具有实用性的专业大模型框架，都是直接使用高级语言实现的，无疑它们也具有加载 AI 模型和参数、执行 AI 推理的能力。在编译这些程序的过程中，高级语言编译器也正确发挥了它们的作用。既然如此，为什么我们还需要 AI 编译器、AI 编程框架这样的东西呢？

我不知道大家有多深入思考过这个问题。实际上，就好像随着电子计算机的发展，我们经历了打孔卡、机器码、汇编语言、C 语言、再到现在的 C++、Rust、Python 等语法形式特别丰富的现代编程语言，我们能写出的功能越来越多，但是真正程序员对计算机的控制其实是越来越弱的。比如说，当使用机器码和汇编编写裸机程序时，我们可以践行冯诺依曼的理念，完全混淆程序与数据的界限，比如直接修改下一个地址的指令然后跳上去跑；当使用 C 语言的时候，我们需要提前规划堆、栈和静态区数据，但是至少对这些规划好的区域还可以随意地读写；但是在 C++ 中引入了类、命名空间和可见性，在 Rust 中程序员受到更多各种各样的约束，而 Python、Java 这样的语言则干脆是跑在虚拟机里。之所以有这样的发展规律，是因为随着计算机的计算能力增长，我们越来越倾向于把程序表达成有利于人类而不是计算机理解的形式。而人类的思维是建立在规律、秩序，或者说是约束、是对自我的限制之上的。而 AI 程序就是编程这件事的一个更加细致、受到更多约束的分支。因此，编写 AI 程序涉及到一种范式转变，就有了我们今天看到的独特的表示和对应的解读和优化工具。

通过导学阶段的课程，我们应该已经知道了 AI 程序的发展历程。今天我们所说的 AI 程序，实际上已经不再指向所有人工智能定义中具有学习和泛化能力的程序，而是特指基于深度神经网络、操作大规模数据且完全无副作用的一种程序。AI 程序这样的特点使得 AI 程序的开发者可以接受相比通用程序的开发者严格地多的限制，以换取开发的便利性。

具体来说，AI 程序具有的 3 个特点使得 AI 程序被设计成以下的模式：

1. 基于深度神经网络，因此 AI 程序具有很强的时序性或者层序性。换句话说，AI 程序中几乎只有顺序执行，控制流极少。因此，AI 程序可以表示为有向无环图（DAG）；
2. 操作大规模数据，因此 AI 程序操作的基本数据类型不是单个变量，而是高维数组。在 AI 程序中，高维数组被称为张量（Tensor）；
3. 完全无副作用，因此 AI 程序可以完全由计算函数调用组成，不需要 IO 模块。在 AI 程序中，函数调用被称为算子（Operator）；

至此，我们就推导出了 AI 程序最常见的一种表示形式，也是 InfiniTensor、RefactorGraph、Onnx 和许多其他 AI 程序格式和 AI 编译器使用的表示形式——计算图。我们来看 [resnet](https://github.com/onnx/models/tree/main/validated/vision/classification/resnet) 的例子（<https://netron.app/>）。

在 netron 可视化系统中，可以看到整个计算图实际上仅由 3 种元素组成。我们从上往下看：

- 最上面的 data 是模型的输入，同理，最下面还有模型的输出；
- 之后是箭头，可以看到这第一个箭头上还标记了一个形状。而且这个箭头是可以点的，我们点上去可以发现它实际上是一个张量；
  - 作为张量，它有数据类型、形状信息，可能还有数据。注意形状可以是未知的，因为除了输入形状，其他张量的形状都是被计算约束的，因此可以由输入张量的形状推导得到。实际上 AI 编译器前端最主要的功能就是形状推导，因此我们作业题也包括大量的形状推导题目；
  - 作为图上的边（Edge），它有源点和目标点信息。注意看 MaxPool 下面的箭头，这两个箭头指示的是同一个张量，指针指上去它们也是一起高亮的。查看内容发现这是一个单个源点两个目标点的边，这样的边表示它由源点计算产生，但是同时被多个目标点使用；
- 这种彩色的方框是图上的节点（Node），同时可以看到它是算子。无论作为算子还是作为节点，它具有类型（type）、模块（module）（这个模块实际上就是命名空间，为了避免类型重名用的）、名字（name）、属性（attribute）、输入（inputs）和输出（outputs）；

如果往下翻，我们会发现整个图是由重复的一些结构组成的。实际上这就是一个循环结构，由于计算图中是不体现控制流的，因此整个循环被展开到许多重复的结构。我这里展示的是最小版本的 resnet18，所以它层数很少。事实上有许多更大型的模型，例如大家都用过的 tiny llama 就是 22 层的 Transformer 模型，我们还有 50 层甚至 80 层的九格模型，可以想象如果对这样的模型可视化的话会得到特别巨大的一个图。

这些东西就组成了一个 resnet 模型。事实上，几乎任何 AI 程序都可以用输入输出、张量、算子这 3 种元素组成。由于这种可行性是我们刚刚根据那 3 个条件推导出来的，因此作为开发者可以信任这个结论。同时也要注意，一旦这 3 个条件不再满足，AI 编译器的这个设计也就立即会从合理简化转变为阻碍。同理，现在的现状是大语言模型经常需要一个独立的推理引擎，而不是接入一般的 AI 编译器，正是因为大语言模型引入了 KV Cache 这一优化手段，这个优化打破了 AI 程序顺序、无状态的假设，从而不再适于用这样的形式来表述了。

把这个描述很直接地映射到代码上，就得到了 InfiniTensor 的抽象。InfiniTensor 中，张量定义在 [tensor_base.h](https://github.com/InfiniTensor/InfiniTensor/blob/master/include/core/tensor_base.h) 和 [tensor.h](https://github.com/InfiniTensor/InfiniTensor/blob/master/include/core/tensor.h)：

```c++
protected:
    int dim;

    DataType dtype;
    vector<WRef<OperatorObj>> targets;
    WRef<OperatorObj> source;
    Blob data;
    Runtime runtime;
```

```c++
private:
    Shape shape;
    size_t _size; // Cache of Π(shape).
```

这些字段存储着刚刚模型中体现出的各项内容：数据类型、形状、数据、输入、输出。可以看到输入是唯一的，输出可以有多个。

算子定义在 [operator.h](https://github.com/InfiniTensor/InfiniTensor/blob/master/include/core/operator.h)：

```c++
protected:
    OpType type;
    TensorVec inputs;
    TensorVec outputs;
    vector<WRef<OperatorObj>> predecessors;
    vector<WRef<OperatorObj>> successors;
```

可以看到这里有算子的类型、输入张量、输出张量，额外还加入了前驱算子和后继算子，这两个字段实际上通过输入输出张量也可以找到，保存一份相当于是一种缓存，在图优化中可能会用到。

最后，在一张[图](https://github.com/InfiniTensor/InfiniTensor/blob/master/include/core/graph.h)中，显然就需要张量列表、算子列表：

```c++
protected:
    Runtime runtime;
    TensorVec tensors;
    OpVec ops;
```

这就是非常直观的设计，可以说没有什么讲的空间。所以 InfiniTensor 其实还真是一个很适合教学的系统，整体设计上就是顺理成章的，只要理解了 AI 程序的一些约束，人人都能写得出来。那么关于 AI 程序的抽象表示就讲完了，今天的课就到这里。

但是真的吗？如果 AI 程序的定义就是这样简单，为什么 InfiniTensor 的张量、算子和图代码却如此复杂呢？可以看到除了这些定义之外还有大量代码。这是因为作为一个带有后端和执行能力的 AI 编译器，显然要做的不仅仅是把 AI 程序表示出来而已。AI 编译器还应该具有两个方面的能力：程序的优化和执行。

优化，意味着对程序结构的变换、以及对参数细节的调整，使得原来的 AI 程序转换成另一个具有不同表示的程序，同时对相同的输入保持输出一致或者误差足够小（因为浮点计算程序要让输出保持绝对一致的条件是很苛刻的）。这意味着图不是能建出来、能遍历访问就完事了，而是必须支持插入删除节点和边。如果是学过数据结构的同学都应该可以意识到，一个有向无环图这样的高级数据结构，建和查的难度和增删的难度完全是两个概念。

而执行，意味着程序需要一个运行时，负责分配和释放各种资源。由于这个后端实际上是解释执行每个算子的，所以运行时还需要一种能够逐个取出要计算的算子、传入正确的参数并计算的结构。以及要执行，就需要每个算子的算法实现，而且对于支持硬件加速的后端来说还需要每个算子在每种硬件上都有对应的实现。这就是 InfiniTensor 以及任何其他 AI 编译器代码量总是如此大的原因。当然另一方面原因是它们都是 C++ 写的，C++ 搞依赖太麻烦了导致大家喜欢造很多轮子而且全写一起。

我们可以看到 [graph.h](https://github.com/InfiniTensor/InfiniTensor/blob/master/include/core/graph.h) 中关于增删算子的代码：

```c++
template <typename T, typename... Args> Ref<T> addOp(Args &&...args) {
    Ref<T> op = infini::make_ref<T>(this, std::forward<Args>(args)...);
    addOperatorAndConnect(op);
    return op;
}
```

```c++
void removeOperator(Operator op) {
    auto it = std::find(ops.begin(), ops.end(), op);
    if (it != ops.end())
        ops.erase(it);
}
```

关于增删张量的代码：

```c++
Tensor addTensor(Shape dim, DataType dtype = DataType::Float32);
Tensor addTensor(const Tensor &tensor);
TensorVec addTensor(const TensorVec &tensors);

void removeTensor(Tensor tensor) {
    auto it = std::find(tensors.begin(), tensors.end(), tensor);
    if (it != tensors.end())
        tensors.erase(it);
}
```

关于分配空间的代码：

```c++
void dataMalloc(bool useNaiveAllocator = false, size_t memPoolSize = 0);
```

本来应该还有用于执行的代码，但是因为设计原因执行被放在了这个 [Runtime](https://github.com/InfiniTensor/InfiniTensor/blob/b9699b0e7a868832203438a9f52a0fda76ad6ecb/include/core/runtime.h#L58) 里：

```c++
/**
 * @brief Execute a graph.
 *
 * @param graph
 * @param tune If there is no performance record, whether to tune it. These
 * can be independent method.
 * @param profiling Whether to print breakdown of time
 */
virtual void run(const Graph &graph, bool tune = false,
                 bool profiling = false) const = 0;
```

实话说我并不完全理解这些东西为啥要这样做。如果完全理解的话我也不会再去写 RefactorGraph 了。Anyway，这些代码总归是都在的，需要的同学可以自己来看。

到现在为止，总结一下，我们已经知道了为什么 AI 程序与通用编程遵循不同的范式，从而需要更适用的 AI 编译器。以及为什么 AI 程序很简单，AI 编译器却不简单。今天要讲的关于 AI 程序的概念和 InfiniTensor 实现的内容就是这些。

（提问时间）

好的，那么讲到现在，不知道大家是否觉得 AI 程序在实现中被表示为这个样子有些奇怪。用大量智能指针把图上的各个元素连接起来，不失为一种可行的方法，但这种表示只有利于改而不利于增删查。由于在存储结构中包含大量的冗余和缓存，对这种图表示进行增删需要大量的关联操作。例如，要从图中移除一个节点，意味着不但要从图的节点列表中移除节点，而且要移除节点输入输出中对节点的引用，还要移除节点输入输出的输入输出对节点的前驱后继引用。同时，要窥见图的拓扑全貌，则必须在大量相互连接的对象之间使用复杂的方法跳跃，要实现灵活的查找几乎必须依赖递归实现的图遍历算法，局部性相当差，不利于现代 CPU 处理。

因此，今天要讲的第二个部分是关于 RefactorGraph 这个项目是如何 refactor graph，来改善这些问题的。
