# 张量及其他介绍

大家好，后面的时间由我来给大家介绍我们的一些工作。

首先我想先做个自我介绍。我 2022 年加入启元实验室，一开始也是跟着陈渝老师做 OS kernel 的，我这段时间做的东西比如：

- [这个](https://github.com/YdrMaster/rCore-Tutorial-in-single-workspace)
- [或者这个](https://github.com/YdrMaster/fast-trap)
- [这些](https://crates.io/me/crates?sort=downloads)

可能大家还见过。[RustSBI](https://github.com/rustsbi/rustsbi-qemu) 第二阶段大家肯定都用了，我参与的也挺多的。AI 这个东西我也是去年才接触的，所以我其实也未必比大家知道的多，估计在场也有些就是学 AI 的硕博，那肯定都比我懂得多。我基本上就是个写代码的，所以后面这些课程对着代码讲的我可能能讲一下，别的我也是跟大家一起学。

今天相当于是我们的导论课，给大家介绍的是 AI 的发展历程，以及这两年爆火的大模型的来龙去脉。我这里则要深入细节，给大家介绍一下基于深度神经网络的人工智能用到的数据在计算机里的最基础的表示，张量。

大家现在已经知道了当代的人工智能意味着大数据大算力，这些海量的数据是怎么被编码的呢？可能图灵或者乔姆斯基这些上一代的大师会想象包含世界知识的高度复杂的数据也应该具有高度复杂的结构。然而现在的大规模深度神经网络的数据在人类看来却恰恰相反，是高度同质，高度非结构化的。当然这种非结构化实际上并不是没有结构，而是人类的理性思维难以感知这种结构。我们这里就不谈那些理论，从结果上来看，张量就是对这种非结构化数据的一种有效的表示。

从开发者的视角看，张量就是高维数组在深度神经网络程序中的别名。例如 C 语言中这样表示一个数组：

```c
T arr[m][n];
```

读作“m 行 n 列的 T 数组 arr”，或者“m 乘 n 的二维数组 arr”。从这个声明中，我们可以看到这个数组最基本的信息，数据类型 T，形状 `m x n`，维数 2。同时我们可以意识到当我们读写这个数组时，我们实际上读写的是一个 `m x n x sizeof(T)` 的连续存储区域。

在深度神经网络中，最基本的张量定义就是将这个数组动态化起来的一种数据结构，如果用 Rust 来表示大概是这样：

```rust
struct Tensor {
    data_type: DataType,
    shape: Vec<usize>,
    data: Vec<u8>,
}
```

> 当然这只是随便写的一个 Naive 的表示。

这里 `DataType` 是一个包含所有支持类型的枚举。`shape`/`data` 就不用再解释了吧。

事实上，有不少真实可用的深度神经网络程序里真是这样定义张量的，比如[这个](https://github.com/InfiniTensor/RefactorGraph/blob/master/src/04kernel/include/kernel/tensor.h)。这个也是我写的用来跑小模型的，c++，可以看到这个张量定义的主体就是数据类型、形状和数据。

在深度神经网络程序中，大部分数据实际上会直接统一到这样的张量定义：

比如：

- 数字，作为变量可能称为“代数类型变量”，在深度学习程序中称为“标量”，表示为一个形状为空——或者说阶数为 0——的张量；

  > BTW，高维数组的维数在张量中称为“阶数”（`order`）。

- 数组，在深度学习程序中称为“向量”，表示为一个阶数为 1 的张量；
- 二维数组，在深度学习程序中称为“矩阵”，表示为一个阶数为 2 的张量；
- 3 阶到 5 阶的张量在深度神经网络程序中都很常见，尤其 4 阶张量在卷积神经网络中非常常见，以至于英伟达提供的 GPU 算子库 cuDNN 中直接为 4 阶张量的初始化提供了[专门的 API](https://docs.nvidia.com/deeplearning/cudnn/latest/api/cudnn-ops-library.html#cudnnsettensor4ddescriptor)；
- 更大阶数的张量也偶尔出现；

这个就是张量最基本的定义，到这里后面大家学习模型结构或者看一些简单的神经网络程序应该就没有障碍了。这里特别推荐大家看一下 K 佬的 [llama2.c](https://github.com/karpathy/llama2.c)，现在我们的 InfiniLM 就是在这个项目的基础上做的。看不惯 C 语言的话也可以看我以前照着写的 [rust](https://github.com/YdrMaster/llama2.rs)。

---

但是，InfiniLM 里的张量定义更复杂一些，更复杂的定义来源于深度网络程序的现实需求。这就涉及到张量和多维数组在用法上的不同。大家都不是第一天学 Rust 了，一定知道 [newtype 惯用法](https://doc.rust-lang.org/rust-by-example/generics/new_types.html)。newtype 惯用法实际上就是在说数据的表示不一定和它的本质一致，或者说数据的语义和存储方式没有那么相关。

同理，张量的形状对于许多计算来说是具有语义的，这种语义会决定计算的方式。例如大家都学过矩阵乘法：

```plaintext
(M x N) <- (M x K) x (K x N)
```

这里出现的 3 个 2 阶张量它们各自的阶数和形状就决定了这个矩阵乘法的计算方式。但是对于基本的张量定义来说，形状和存储方式是一一对应的，所以为了得到需要的张量形状而对张量进行变换的时候就必须同时变换底层的数据表示。由于张量经常在很简单的形状下管理着海量数据，比如在 InfiniLM 中就会出现 4096x4096 的 f16 数组，变换它就是在挪动 32MiB 字节，这是非常低效的。所以我们就需要一种表示把张量的逻辑形状和存储方式分离，然后变换的时候尽量只改变逻辑形状，只有在必要的时候才变换存储方式。我们采用的是一种经典的方式，就是为张量引入步长（`strides`）字段：

```rust
struct Tensor {
    data_type: DataType,
    shape: Vec<usize>,
    strides: Vec<isize>,
    data: Vec<u8>,
}
```

步长 `strides` 是一个长度等于阶数的数组，表示每个维度上两个数据的间隔。例如一个 C 二维数组：

```c
T arr[m][n];
```

它的步长就是 `(n, 1)`，因为在索引在长度为 `m` 的这个维度上每增加 1，就意味着数据的地址跳过了 `n` 个元素，用矩阵的语言描述就是矩阵的两行之间隔着一行的长度——也就是列数——那么多个元素。而索引在长度为 `n` 的这个维度上每增加 1，地址也只增加了 1 个元素。这就是所谓行优先存储的矩阵。

如果我接下来需要的矩阵运算需要我转置这个矩阵，对于基本的张量表示来说真的需要挪动矩阵中所有的数据，对于 4K x 4K 的矩阵这需要挪动 16M=1677'7216 个数据。而引入步长之后我只需要直接把形状和步长的 4 个数字交换一下就完成了：

|     | 原矩阵 | 转置矩阵
| :-: | :-: | :-:
| 形状 | M x N | N x M
| 步长 | N \| 1 | 1 \| N

如果认为这个修改算是优化加速的话，加速比达到 (16MiB+2) / 4 = 419'4304.5 倍，所以大家知道某些数据是怎么刷出来的了吧。

---

在 InfiniLM 中，[张量的定义](https://github.com/InfiniTensor/InfiniLM/blob/main/tensor/src/tensor.rs#L12)还包含一个模板参数。这是因为我们的推理引擎同时支持 CPU 推理和 GPU 推理，以后还会支持更多加速硬件，数据在不同的硬件上保存在不同的存储空间中。在我们引擎里这些不同的表示都是通过类型来区分的，保证了安全性。另外，这里的 `pattern` 字段称作“增广偏移的步长”，它比刚才讲的 `strides` 步长长一个数字，这是为了支持更多种类的变换，这里我就不一一展开了，大家可以直接看代码的 tensor crate 来了解这些变换实际是怎么操作的（还挺巧妙的）。最后，`fmt` 这个模块里有一个打印各种变换张量的[测试](https://github.com/InfiniTensor/InfiniLM/blob/main/tensor/src/fmt.rs#L144)，大家可以直接运行这个测试直观理解每个变换都在干啥。
