# 服务层代码讲解

大家好，今天还是由我讲代码。今天我会带着大家通读一遍 InfiniLM 的服务层代码，希望从明天开始大家就可以动手调试和修改代码，甚至为项目做些贡献了。我推荐大家可以把自己本地的项目也打开，讲的中途我也会给大家留一些空当，大家可以看自己本地的代码复习或者提问。

因为可能需要介绍一些单机分布式推理的行为，所以我今天是在启元的服务器上给大家演示。

```bash
nvidia-smi
```

大家可以看到我们这台服务器插了 10 张 A100 80G，并且 9 号卡占用 50 多 G，这个卡上部署的就是启元内网上的一个基于百亿模型的推理服务。上周我又改了一个 Bug 之后现在运行还算挺稳定的了。所以我也鼓励大家可以在自己电脑上尝试部署一个服务，一方面帮我调试一下看看还有没有 Bug，如果能找到一些 Bug 甚至帮我改掉的话可以直接向我们仓库贡献代码。另一方面大家也可以尝试写一些命令行的、桌面的或者浏览器的推理客户端，如果做的比较美观或者有什么创意功能的话也可以吸纳到我们组织里。

---

出于方便理解的考虑，我们今天的讲解会从 xtask 的 generate 指令实现开始，然后逐步深入到整个服务层的几个 crate，介绍推理的一些外围工作，以及服务的调度原理。

首先给不太熟悉 xtask 模式的同学介绍一下这个东西。xtask 是由 Github 上的[这个仓库](https://github.com/matklad/cargo-xtask)提出的一种向项目添加自动化能力的设计模式。它的原理在 cargo workspace 里添加一个 binary crate（为了体现 xtask 模式我一般会把这个 crate 叫做 xtask，但是它的名字实际上无所谓），把这个 binary 实现成一个命令行应用程序，然后通过配置 .cargo/config.toml 文件来将一些 cargo 命令转发给这个 binary。这样就可以在这个 binary 里直接用 rust 实现各种自动化功能。

传统上包括 rcore-Tutorial 在内的很多项目会用 *.sh 脚本和 Makefile 来实现自动化，比如大家在 rcore-Tutorial 里可能经常会打：

```bash
cd os
make run
```

但是用 Bash 和 Makefile 实现自动化有 3 个重要的问题，首先是不跨平台，在原生的 Windows 是用不了 Makefile 的，当然 WSL 可以，但是隔了一层总会觉得不爽。其次是包括 Makefile 在内的这些配置脚本都有自己的奇怪语法，这些语法对于不熟悉的人来说实在是相当难维护，而且基本没什么 IDE 或者编辑器插件能像辅助你写高级语言那样帮你写脚本，导致随着配置变得越来越复杂，脚本维护起来就越来越困难。第三是这些脚本跟 Rust 本身的生态是割裂的，它最多做到编译执行你的应用程序，但是没办法连接到你的库做些什么事。

因为 xtask 模式指示 Rust 应用程序的一种调用范式，所以它自然而然地解决了这 3 个问题。它在源码级别上跨平台，所以我这个项目可以直接 Windows 运行，而且各个平台体验完全一致，不用记住好几套命令；它用 Rust 语言实现，在 vscode 里写起来舒服，而且可以接入 Rust 生态，直接从 crates.io 上获得依赖，频繁使用的代码也可以发布到 crates.io。我之前还写过[这个](https://crates.io/crates/os-xtask-utils)，大家自己做内核的时候可以尝试一下。最后在 InfiniLM 这样的项目里，你真正的代码实际上是写成 library crate 的，可以把应用程序做到 xtask 里，用起来也挺方便。

---

言归正传，我们先来回顾一下 xtask generate 的功能。

```plaintext
cargo generate --model ../TinyLlama-1.1B-Chat-v1.0_F16/ --prompt "Once upon a time," --nvidia 1
```

这个命令以输入的提示词为开头续写一个故事。我不知道有多少同学已经看过这块代码了？总之今天我带着大家过一遍。

我们看到 xtask/src/main.rs 里的 main 函数。我这里的命令行参数解析是使用 [clap](https://crates.io/crates/clap) 这个 crate 实现的，这个库是 Rust 生态里特别经典的一个实现。今天我就不涉及它的用法了，需要的同学可以自己去看它的文档。总之我们通过这个库可以解析参数并且分发到指定的子任务上，我们直接来看 generate。

当子命令是 generate 的时候，所有参数会解析然后保存在这个 `GenerateArgs` 结构体里，调用 `args.run()`。点进这个 `args.run()` 可以发现这个方法是实现在 Task Trait 里的。这个 Task Trait 实际上是要求实现提供两个功能，它基于这两个功能实现调用。回来看 main 函数的话容易观察到 generate、chat 和 service 都调用了 run，可见它们应该是通过提供不同的方法实现来完成多态，共用了一套调用逻辑。

继续看这个 `Task::run` 的实现：第一步从用 `self.inference` 里调用日志器初始化，然后启动 tokio 运行时。

初始化日志器用的是 xtask 传的 --log 参数。现在主流的日志参数其实是通过环境变量来传递，所以 [crates.io 上搜索 logger](https://crates.io/search?q=logger&sort=downloads) 下载量最大的会是这个自动读取环境变量的 env_logger，不过我觉得用 clap 传参数更方便，而且在不同的 shell 里设置环境变量的语法不同，传参体验更一致一些。

tokio 是目前 Rust 应用程序级异步运行时的事实标准，以前还有个 [async-std](https://crates.io/crates/async-std) 但是好久不更新了，好像已经死掉了。我们这里引入异步运行时是因为调度多用户并发推理以及 web 服务的时候协程可以减少很多线程切换开销。不过核心的推理任务实际上是计算非常密集的，很容易占满推理硬件，所以一会看到里面推理任务的时候能看到它实际上是单线程的。有些接触过 tokio 的同学可能会习惯于 tokio::main 这样的特性直接实现一个异步运行时控制的 main 函数。这里因为我们不是所有任务都带异步，所以我是按需手动启动的。

接下来如果感知到 cuda 环境就调一下 cuda 初始化。cuda 生态我们安排在下节课讲，现在如果不熟悉这个东西的话直接略过就行。但是我需要提一下这个 cfg 条件编译是怎么做的。xtask 里面有一个 build.rs，这个东西在 cargo 环境里叫做构建脚本，这个的具体用法大家可以自己[必应一下](https://doc.rust-lang.org/cargo/reference/build-scripts.html)。现在就大概看一下实现。首先先检查是否配置了一个叫做 `nvidia` 的 feature，没有直接退；然后尝试在环境里找 cuda 的安装目录，如果能找到就给 rustc 配置一个 cfg 叫做 `detected_cuda`。所以这里可以写 `#[cfg(detected_cuda)]`。有些群友反映这个地方会报一堆 `detected_cuda` 不存在的警告，我不理解，因为 cfg 本来就是根据里面的东西是否存在来决定要不要编译它控制的代码块的，不存在当然是它的常态了。而且我也复现不了这个问题，所以我只能怀疑是 Rust 版本问题。如果一直报这个错的同学可以找点别的环境测试一下看看影响因素到底是什么，以后再交流。

下一步这个是从 `inference` 取出 nvidia 配置，这里如果没配置就是 CPU，配置一项就是 NV 单卡，配置多项就是 NV 分布式。然后判断模型类型。模型类型参数是前几天才刚加上的，因为我们刚刚支持了一个非 Llama 的模型，mixtral MOE 模型。这里暂时用一个参数来区分，后续会改成直接读取模型参数自动判断。

现在只看 Llama 这个分支，下一步判断 nv 参数是哪一种。我们这个项目里同一个模型在不同的硬件配置下各自有一套代码，因为不同的硬件上模型在加载和计算上有各种各样的差异，不排除以后硬件或者模型多了再想办法做一个抽象层来复用代码。

模型类型和硬件参数决定了使用哪种模型实现，然后就可以把这个模型类型传递给这个 `typed` 方法，然后把返回值传递给 tokio 运行时的 `block_on` 方法。可能很多同学没见过这种写法。这涉及到一些 Rust async 的实现原理，Rust 里所谓 `async fn` 实际上就是返回 `Future` 的函数的语法糖，所以这个没有返回值的 `async fn` 的返回值可以传递给 `block_on` 这个接受 `Future` 的函数。几乎所有异步运行时都会包含这个名为 `block_on` 的函数。这个函数通常会被描述为同步世界和异步世界的桥梁。它的功能是阻塞当前线程等待一个异步任务完成。

我们先看完 run 函数的最后两句话，包含具体工作的 `typed` 执行完后，对于 cuda 需要在所有卡上做个同步，最后关闭 tokio 运行时，优雅退出。

这就是 run 的完整逻辑，总结一下，它实现了初始化运行时环境、选择模型和算力硬件，然后把选中的模型转换成类型参数传给 `typed` 调用具体任务，最后清理运行时环境退出。

---

下一步我们进入 generate.rs 看文本生成的具体实现。这个文件代码相当少，全算上才 50 多行。可以看到生成参数就是推理参数加上提示词和最大解码步数。`inference` 实现就是直接把推理参数返出去，没啥可说的。

`typed` 里面，首先用推理参数里的模型路径和模型类型相关的元数据来加载推理服务抽象，得到服务对象和承载推理服务的异步任务。然后把提示词打印出来。这里我用 CPU 跑一下，CPU 跑得慢能明显看到提示词是先打印的，稍微等了一下第一个词才出来。

```plaintext
cargo generate --model ../TinyLlama-1.1B-Chat-v1.0_F16/ --prompt "Once upon a time,"
```

然后取一个最大步数，不传就是无限大。一开始步数是 0。然后调用推理服务里的 generate 方法，传入提示词和采样参数，得到了一个生成器对象。推理出来的一个一个词是循环调用生成器对象的异步解码方法得到的。把得到的东西转义打印出来，一直到句子结束或者达到最大步数。最后打印一个平均时间。这些代码都很容易理解，没什么需要解释的。

下一步我们可以离开 xtask，进入 service crate 了。所以我们先休息两分钟，给大家一些时间回去看一下讲过的这些代码。有问题的话也可以直接开麦提问。

---

接下来我们进入 service crate。首先看 `Service::load`。

可以看到传入的 model_dir 使用了 4 次，分别用于加载模型 M，tokenizer、normalizer 和 template。tokenizer 和 template 上节课讲过了，分别是分词器和对话模板，这个 normalizer 在配置中属于是 tokenizer 的一部分。给大家看一下 TinyLlama 里面的这些配置，我这里打开的是一个没有 cast 过的原版 TinyLlama Chat 模型：

真正的 tokenizer 是这个 tokenizer.model，这是一个二进制文件，里面有分词器需要的所有参数。

还有一堆名字里有 tokenizer 或者 tokens 的 json 文件。

先看这个 tokenizer.json。normalizer 就定义在这个文件里。可以看到这里定义了两项 normalizer，先 prepend 一个下划线，然后是把所有空格 replace 成下划线。注意这里的这个下划线是一个特殊字符。我在搜索里搜一个下划线，可以看到能搜到名字里的真·下划线，搜不到这种下划线。基于 Llama 标准分词器训练的模型都是把这个特殊 Unicode 下划线当做空格来训练的，所以它在 normalizer 里要把空格换成这个，解码的时候再换回来。但是如果文本里真的出现这个下划线的话，它是没能力转义的，就会当作空格处理。我来演示一下：

```plaintext
cargo generate --model ../TinyLlama-1.1B-Chat-v1.0_F16/ --prompt "Once▁upon▁a▁time," --nvidia 1
```

可以看到这个推出来和空格的版本是一模一样的。所以说大模型表面上鲜花着锦，其实即使是顶尖的工作细节也很坑，一点都不严谨。

这个 prepend 起到的作用是在整个句子的前面加一个空格。为啥要这么干呢，我们可以翻一下这个文件，这个 json 文件里也有一份完整的词表。搜索 `▁` 这个特殊字符，可以看到这里有 `▁t`、`▁a`、`▁th`、`▁s`、`▁d`……很多这样的词。包括大写版本的 `▁A`、`▁B` 也有。这是因为英文里是用空格来分词的，所以显然空格后面跟任何字母或者任何前缀都会非常常见，比如一些常见的前缀 `▁en`、`▁dis` 也是有的，但是后缀 `▁tion`、`▁ture` 就不会出现。但是一整段话的开头通常是没有空格的，所以如果直接分词就会导致第一个词在开头时得到的 token 和同一个词在这段话中间得到的 token 大相径庭，这个对于推理非常不利。所以要在 normalizer 的层级把开头的空格加上。

最后是 template，它出现在 tokenizer_config.json 这个文件里。这东西干脆就是一个脚本，我把它拷出来整理了一下，其实是这样：

```plaintext
for message in messages
    if message['role'] == 'user'
        '<|user|>\n' + message['content'] + eos_token
    elif message['role'] == 'system'
        '<|system|>\n' + message['content'] + eos_token
    elif message['role'] == 'assistant'
        '<|assistant|>\n'  + message['content'] + eos_token
    endif
    if loop.last and add_generation_prompt
        '<|assistant|>'
    endif
endfor
```

它就是在说拿到一个消息列表，根据里面每一项的身份怎么给它的内容修饰一个标签上去，最后一个消息之后该 ai 说话，所以再补充一个 ai 的标签作为开头。

以上讲到的就是 tokenizer、normalizer 和 template 的原理。

现在我们回过来看一下代码里这几个函数（template、normalizer、tokenizer），可以发现这里面其实根本没有读配置文件，而是直接根据传入的路径名字或者包含的文件来选择用哪种实现，每种实现的内容就是直接硬编码的。这是因为这个配置文件并没有一个规范写法，基本上就是训练的人随便写一下，推理引擎根据想支持的模型是怎么写的具体去适配。所以如果有同学愿意总结一下现在的模型都是咋写的，实现个更通用的会读配置文件的版本，可以发 pr 我来合并。

好的，再回到 `Service::load`，可以看到它加载模型，然后把模型放进了一个叫做 `Dispatcher` 调度器的类型里，并把这个调度器分享给了一个叫做 `ServiceComponent` 服务组件的对象。这个服务组件还带有 template、normalizer 和 tokenizer 这三个东西。很容易想到服务组件其实就是从模型配置和权重加载出来的所有东西，这些东西都是不可变的，会在各种地方用到，所以放在一个 `Arc` 里。另外 Self 里还有一个默认采样参数，这个参数是对每个 Service 可以配置的，所以不放在 `Arc` 里。

返回的第二个对象是 tokio 调用 spawn_blocking 得到的。这个其实是让 tokio 开了一个线程跑这个 `handle.run()`。spawn_blocking 这个函数是用来在 tokio 运行时里执行阻塞操作的。因为 tokio 是一个协程运行时，它默认的线程池是执行协程用的，只会在协程的挂起点主动切换任务。所以长期运行的重计算任务是不能放到协程运行时里跑的，那样会直接堵死线程池里的一个线程，严重影响协程的调度能力。这个 spawn_blocking 就是让 tokio 运行时在一个隔离的专门跑阻塞任务的线程池里取一个线程执行这种长期任务。

我们现在看到有一个对象叫做 `Dispatcher` 调度器，它存入了模型，并且还需要阻塞地 `run`，容易想到它就是真正执行推理的那个对象。点进这个 run，可以看到它其实就是一个循环，不断地从这个 `batcher` 批处理器里调用出队操作取一系列推理任务，然后把这些任务变成各种形式喂给 `self.model` 这个模型里的一系列推理方法，先是 token_embed 词嵌入；接着 `forward` 前向传播，也就是神经网络的推理；然后 `decode` 解码，把 `hidden_state` 转换成概率分布；最后 `sample` 从概率分布里采样得到一个词。得到词之后实际上推理硬件就空闲了，所以这里启动了另一个阻塞任务负责把得到的词发送回每个任务的输出队列，这个发送工作异步进行，大模型本身可以直接回去从批处理器拿下一批任务，这样就提高了推理硬件的利用率，从而提高了服务整体的吞吐量。

我们之所以能够通过 self.model 调用到这些方法，是因为实现调度器的时候做了这个类型约束。self.model 实际上就是这个 M，它被约束成必须实现 CausaLM 特性，这些方法也是由 CausalLM 提供的。所以接下来我们会进入 CausalLM 因果语言模型的定义，和上次课讲的 Llama 模型结构对应上，看看推理服务调度因果语言模型的每个运算步骤都做了什么，又需要哪些参数。现在我们休息几分钟，给大家一些看代码和提问的时间。

---

上次课已经给大家介绍了因果语言模型 CausalLM。现在来给因果语言模型下个定义。因果语言模型是一类预训练模型，它们的目标是从样本中学习语言的逻辑结构，从而根据前文预测下一个词的概率分布。causallm 这个 crate 定义了一种狭义的因果语言模型，具有以下特点：

1. 基于词（`token`）和词表（`vocab`）量化语言，词表有且仅有一个终结符（`eos`）；
2. 每层包含一次自注意力机制（`self-attention`）计算，能够感知一个有限的上下文容量（`max seq len`），并支持 KV Cache 加速；
3. 若存在某段文本 `[..p]` 区间的上文缓存，可通过输入 `[p..][..seq_len]` 区间的文本并估计 `[k..=p+seq_len]` 区间的词概率密度（k > p）。通常情况下 `k = p+seq_len`，即认为不可修改已存在的词，只预测新增的词；

   ```plaintext
   | t0 t1 t2 ... tp-1 | tp tp+1 ... tk-1 | tk ... tn-1 | tn |
   |<---- kv cache --->|<--------- query -|------------>|    |
                                          |<---- logits ---->|
   ```

4. 对于每个词表概率密度，采用随机采样法得到其中的一个词（贪心采样是 `top-k = 1` 的随机采样）；

这几条性质里，第四条可能和大家的认识不太一样。这段话说的是，模型历史上记录了 p 长度的 kv 缓存，这一次输入 seq_len 个 token，所以模型现在已知一个 p+seq_len 长度的 token 序列，也就是图上的 n。然后它可以解码得到最多 seq_len 个词的概率分布。这个也比较好理解，上节课讲过自注意力计算包括一个因果遮罩的 softmax 运算，所以这一轮输入的每个词都只能看到自己之前的词，所以每轮推理的每个词也就可以独立地推出自己的下一个词的概率分布。绝大多数情况下我们都认为前 n 个词就是固定的，只需要解码新的那一个词的概率分布，而且因为我们每次只解码一个词，所以下一轮的输入也只有一个词，所以下一轮也就只能解码一个词。但是还是有一种优化技术叫做投机采样，应用这个技术的时候会出现每轮输入多个词解码多个词的需求，所以这里对因果语言模型做了更宽泛的定义。下周我们会给大家详细介绍每个算子的实现，可能那节课之后大家可以对这里为什么能这样定义有更好的理解。

我们进来看代码。lib.rs 定义了两个重要的 trait。Model 表示一个模型怎样从文件系统加载。CausalLM 表示加载出来的参数和权重怎样用于推理。这里有 6 个重要的方法，文档都写的比较详细了。`new cache` 用于创建空白的 kv cache；`duplicate_cache` 用于从一个上文缓存中复制有效的部分到一个新的上文缓存；`token_embed` 用于词嵌入；`forward` 用于前向传播；`decode` 用于解码；`sample` 用于采样。这些方法目前都是在一个控制流里同步、顺序地调用的，但是其实由于各个阶段对于硬件的利用率不同，对于并发数比较多的情况，更好的实现是把这些阶段流水起来，有可能前一轮推理的采样阶段和后一轮推理的词嵌入可以同时进行之类的，有可能能达到更高的吞吐量。学术上也有相关的前沿研究，有兴趣做性能优化的同学以后可以找论文读一读，或者自己尝试一下。

因为一轮推理被分成了这些阶段，为了在阶段之间传递中间变量，就需要一个关联类型 `Storage`。这个关联类型也使得 CausalLM 不能创建 dyn trait obj，所以 xtask 的 `Task::run` 需要实现为传递模板参数的形式。

我们来看这些方法的参数。

`token_embed` 输入的 queries 是一个 token 迭代器。我们在调度的时候，每次从会队列里取出多个请求，但是只需要调用一次 token_embed，因为词嵌入计算对每个词都是独立的，我不需要区分查询的每个词是属于哪个上下文，所以实现上可以直接把所有请求的输入拍平一起传给词嵌入方法。

`forward` 运算需要传入每个请求的上文缓存才能计算自注意力，所以这里的这个迭代器迭代的是每一个请求。类型是这个 `QueryContext`。这个类型不难理解，它就是一个上文缓存张量和一个查询的词范围。也就是图上的 p..n 这个范围。比较奇怪的是这个上文缓存外面套了一个 Option。这个 Option 是为了调度时实现灵活取消用的。也就是说一个任务已经放在任务队列里排队之后，可以通过把上文缓存取走的方式取消掉这个任务。被取走上文的请求也就不会计算自注意力了。这样服务的调度会更灵活，取消响应更快，无效计算更少。

`decode` 运算需要把模型的输出转换成概率分布，刚刚讲过每个请求解码多少个词的概率分布是可选的，这里就是通过 `DecodingMeta` 这个类型来设置。

`sample` 运算是从概率分布里采样一个词，所以输入是每个需要采样的词的采样参数。这里的 num_decode 和解码的 num_decode 是一样的。采样参数里就是随机采样的温度、top-k 和 top-p。
