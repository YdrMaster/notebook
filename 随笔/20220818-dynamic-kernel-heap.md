# 动态内核堆

- 关键词：设计，内存管理

> 本文经过更新，较新的内容在上方。

> `^x^` 是上角标语法，Github 不支持渲染。

## 2022-08-20

把内核堆整个放在内核动态区这个设计草率了，没这么麻烦。实际内核需要的不是两个分配器（物理页帧分配器、内核堆分配器），而是一套完整的（物理）内存管理机制。同时，为了效率，将整个分配过程变为多级内存池，在不同粒度分配、缓存、回收。例如：

1. 整个物理内存空间被一个高效但粗糙的分配器管理（Linux 是伙伴分配器^1^），其分配的粒度大约在物理页的层次^2^；
2. 一个稍微细粒度的分配器负责为内核动态分配物理内存（Linux 是 slab 分配器^3^），内核可以直接线性映射整个物理内存，动态分配也只是逻辑上的所有权分配，可以不动页表，因此效率更高。这个分配器从全局分配器获得大块再细分，同时直接分配跨硬件线程的内核动态对象；
3. 多核环境下，内核的每个硬件线程上会有一个上下文，可以为每个上下文配置一个 thread-local 动态分配器，从内核堆分配器获得大块再在硬件线程内分配。这样的好处是其分配可以不加锁^4^；
4. 每个进程的用户态肯定会有个 libc 提供或自定义的分配器，但内核也可以在进程级别缓存一些。这个分配器从全局分配器获得大块；

---

> 1. 伙伴分配器挺好的。zCore 里是 bitmap-allocator 上了一把大锁，效率低。zCore 内核堆是伙伴分配器但每级内是链表，可以优化为平衡二叉树。
> 2. 顶级分配器以物理页帧为单位分配，听起来很直观，但这是必然的吗？我没找到这个必然性在哪。假设这个顶级分配器是伙伴分配器，看起来约束应该是阶数而不是最小的单元大小。显然物理内存每大一倍伙伴分配器就要加一阶，分配性能就降低一点（多核加锁就更严重！）与其在不同物理内存容量的硬件就获得不同的性能，不如在保证相同性能分配不同粒度。反正物理内存更大就更能容忍碎片。
> 3. 我还没来得及看实现，无法评价；
> 4. Linux 似乎没有这个设计？也可能到这里就叫内存池而不称作“分配”。

---

多级分配器完全可以具有相同的接口定义，不同的只有复杂度和分配粒度，这样这些分配器实际上构成了一个递归结构，可以任意选择实现。实现思路可以类似现有的 system allocator，直接侵入那些待分配的内存区域记录元数据。这其实造成了对分配器粒度的约束：分配器不可能分配比自己的节点元数据更小的内存块^1^。

实际上越高级的分配器越该用通用实现，而低级的可以用复杂实现。因为越低级发生分配就越频繁，并且越能用上特定环境的先验信息。参考 JVM 的垃圾回收，可以用上着色、图分析等等实现很奇妙的性能。而顶层就使用一些简单的、复杂度就是低的算法就对了，O(logN) 一定比 O(N) 好。

---

1. 可能常见的形式是几个 `usize`？或者可以在最小的粒度用位图。

---

### 参考资料

- [伙伴分配器原理](https://zhou-yuxin.github.io/articles/2017/%E4%BC%99%E4%BC%B4%E5%88%86%E9%85%8D%E5%99%A8%EF%BC%88buddy%20allocator%EF%BC%89/index.html)
- [位图分配器原理](https://zhou-yuxin.github.io/articles/2017/%E9%AB%98%E6%95%88%E9%80%9A%E7%94%A8bitmap/index.html)
  > 可以直接看[王润基的实现](https://github.com/rcore-os/bitmap-allocator)，但接口设计有问题。另外每级使用 `u16` 应该是为了方便对齐到 2^8^，但这有意义？我看不如直接 `usize` 降低层数。另外这个没做 index_hint 优化，每次遍历性能太差了；另外这个真的不适合多核环境，只能一把大锁。详细讨论：
  >
  > 先说结论：关于位图并行化尝试的结论就是只能把一把大锁在最高层拆成每个位一把锁（用一个位的段落锁），不可能再降低锁粒度了，实际应该根本没用。原因是位图用一个比特位表示下辖整个子空间的状态，而分配回收两种操作会以不同的方向影响上层比特位，分配必定导致变为占用状态，和之前状态无关；回收只会发生在占用时，但可能影响上层也可能不影响。这意味着从上层的角度看多个分配可以同时进行，而多个回收或分配和回收必须互斥。这像是读写锁，但读写锁是不可能只用 1 比特实现的。而位图的本质就是一个最细粒度块只用 1 比特。所以：
  >
  > 1. 位图不可能具有细粒度锁
  > 2. 位图根本不适合作为全局分配器

## 2022-08-18

### 目的

现在 _Core 的内核堆总是直接占用固定内存，既导致比较大的浪费（外碎片），有时候又会出现不够用的情况。其实内核堆完全可以动态起来，按需分配，和用户程序复用物理页帧（Linux 也是这么做的）。

### 设计

内核的内存管理涉及到内核的 4 方面功能：物理页帧分配、页表管理、内核堆分配和进程虚存管理。根据三者关系又有两种方式实现动态内核堆。

#### 机制

物理页帧分配按需从全局可用的物理内存空间\*中找到可用的页帧提供给内核其他功能使用。由于管理的单位是页（通常 4 KiB）且不涉及权限控制，所以通常相对简单。比如使用 BitMap 方式，可以用一个位表示一个页，这样每个字节就能管理 32 KiB 物理内存，完全控制 256 GiB 空间才需要 8 MiB 额外空间，效率很高。

---

> - 物理页帧分配器下可以包括也可以不包括已经被固定占用的 MMIO 段、SBI 段和内核链接段，反正包括了就是永远不释放。设定问题不影响结果。

---

常见的实现方式就包括 BitMap 和伙伴分配器等，BitMap 的优点是简单，本身不需要额外内存；缺点是必须静态决定管理多大空间，如果实际管理的很小就会造成浪费。另外只适合单个 bit 分配小页，分配大页或多个连续页则比较麻烦。其他方案则根据设计可以快速分配多个页、大页、需要特定对齐的页，但实现比较难。最难的地方在于往往这些方案自身占用的空间也是动态的，如果要使用内核堆，内核堆就不可能反过来使用页帧分配器。最好能设计一些页上数据结构，让物理页帧分配器能自己给自己分配。

页表管理就是和硬件交互，将分配出去的物理页通过硬件页表映射到虚页。映射本身是很容易的，然而管理有困难。页表映射的数据页由于涉及权限管理，通常都要在什么地方记录，并且映射过程中还会产生数量不确定的中间页表（和物理页的连续性、对齐以及大页分配有关），这些显然都需要动态的空间存储，也就是需要内核堆。所以如果内核堆也需要页表映射出来就又产生了矛盾。换句话说，至少内核页表的管理也需要不依赖堆的页上数据结构。

接下来是内核堆。动态的内核堆应该管理着内核虚地址空间的内核动态区，这上面的虚页初始都是没映射的，每次从一个新的页对齐区上分配时就映射一个，每次一个整页释放时就解除映射。堆管理的单位是字节，这样再使用静态的 BitMap 实现就很低效了，而其他方案又需要动态分配。并且这些分配回收过程还都需要和页表联动。

最后是进程虚存管理。这个还算相对容易一些，因为内核的其他部分不会依赖它。只需要先把页帧分配器和堆分配器弄好就行了。

#### 方案

以上其实已经介绍了复杂的方案：必须先设计一些自给自足的、能够高效利用整个物理页帧的数据结构。物理页帧分配器、页表管理器和内核堆分配器都要基于这样的数据结构保存自己，然后作为一个整体向进程虚存管理器提供功能。

实际上，“作为一个整体”这件事提醒了我们，还有另一种方案，就是直接实现一个静态的堆分配器，然后由堆分配器模拟出页帧分配器的行为，共同支持页表管理器，最后再服务于进程虚存管理器。换句话说，将本来应该由物理页帧管理器管理的整个可用物理页区域交给一个以字节为单位的分配器使用，将物理页帧分配描述为分配对齐到 4 KiB 的 4 KiB 字节数组。这样可能效率较低，但确实是一种可行的简化设计，并且依然支持内核堆与进程共享物理内存。
