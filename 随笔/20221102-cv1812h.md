# cv1812h 信息记录

## 华山派开发板

- [公开资料](https://github.com/sophgo/sophpi-huashan)

### 烧写系统

1. 拉仓库
2. `cd mmf-sdk`
3. `. build/cvisetup.sh`
4. `defconfig mars` 查看支持的开发板
5. `defconfig cv1812h_wevb_0007a_emmc` 配 cv1812h + emmc
6. `build_all` 编译
7. install 目录下找到所有 `fip.bin` 和 `*.emmc`，拷贝到一个 FAT32 格式化的 TF 卡
8. 掉电插卡，然后上电，自动烧写
9. 拔卡，重新上电

## TPU

1. 神经网络框架

   > [关于 ONNX](https://zhuanlan.zhihu.com/p/346511883)

2. 神经网络工具链

   1. 模型量化

      > - [资料 1](https://zhuanlan.zhihu.com/p/141641433)
      > - [资料 2](https://zhuanlan.zhihu.com/p/157633225)

   2. [算能 TPU-MLIR](https://tpumlir.org/index.html)

      > **通用架构TPU编译工具链**
      >
      > TPU-MLIR 是一个专注于 AI 芯片的 TPU 编译器开源工程。该工程中提供了一套完整的工具链，用于将不同框架下预训练过的神经网络转化为可以在 TPU 上高效运算的二进制文件 bmodel。

3. 软件架构

   推理应用程序软件可以分为 4 层：

   1. 应用程序
   2. 运行时库
   3. 操作系统
   4. TPU 驱动

   向应用程序传入模型文件和数据以进行推理。

   编译应用程序时需要链接运行时库。不同类别的硬件的区别最高体现在运行时库上，例如 Docker 上编译时链接的应用程序会直接使用 CPU 开线程计算，此时不需要 TPU 驱动。strace 观察到 sample 程序会打开 2000+ 线程。

   如果运行时会使用驱动，操作系统负责提供运行时库和驱动的通信，另外还管理二者需要的资源，例如 DMA 和中断。

   TPU 驱动直接操作 TPU 的寄存器。

4. 驱动开发

   - [ioctl 原理](https://zhuanlan.zhihu.com/p/578501178)
   - [TPU ioctl 请求定义](https://github.com/sophgo/sophpi-huashan/blob/master/mmf-sdk/middleware/v2/include/linux/cvi_tpu_ioctl.h)
