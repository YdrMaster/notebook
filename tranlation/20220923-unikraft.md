# Unikraft：构造快速、专用的幺内核的简单方法

> Unikraft: Fast, Specialized Unikernels the Easy Way

---

> [原文](https://dl.acm.org/doi/pdf/10.1145/3447786.3456248)

## 译文对照表

| 原文 | 译文
| :-: | :-:
| unikernel | 幺内核
| micro-library | 微库

## 摘要

> Abstract

幺内核因在启动时间、吞吐量和内存消耗等方面提供出色的性能而闻名，这里仅举几个指标。然而，它们因使提取这种性能变得困难和极其耗时而臭名昭著，并且需要大量的工程努力才能将应用程序移植到它们上面。我们推出 Unikraft，一个新颖的微库操作系统：（1）完全模块化的操作系统基元，因此很容易定制幺内核并只包括相关的组件；（2）暴露了一套可组合的、面向性能的 API，以便使开发者容易获得高性能。我们使用现成的应用程序（如 nginx、SQLite 和 Redis）进行的评估表明，在 Unikraft 上运行它们，与 Linux 虚拟机相比，性能提高了 1.7-2.7 倍。此外，这些应用的 Unikraft 镜像约为 1MB，运行时需要不到 10MB 的内存，在 VMM 时间之外，启动时间约为 1ms（总启动时间为 3-40 ms）。Unikraft 是 Linux 基金会的一个开源项目，可以在 <https://www.unikraft.org> 找到。

> Unikernels are famous for providing excellent performance in terms of boot times, throughput and memory consumption, to name a few metrics. However, they are infamous for making it hard and extremely time consuming to extract such performance, and for needing significant engineering effort in order to port applications to them. We introduce Unikraft, a novel micro-library OS that (1) fully modularizes OS primitives so that it is easy to customize the unikernel and include only relevant components and (2) exposes a set of composable, performance-oriented APIs in order to make it easy for developers to obtain high performance. Our evaluation using off-the-shelf applications such as nginx, SQLite, and Redis shows that running them on Unikraft results in a 1.7x-2.7x performance improvement compared to Linux guests. In addition, Unikraft images for these apps are around 1MB, require less than 10MB of RAM to run, and boot in around 1ms on top of the VMM time (total boot time 3ms-40ms). Unikraft is a Linux Foundation open source project and can be found at www.unikraft.org.

## 1 引言

> 1 Introduction

可以说，专用化是实现卓越性能的最有效方式，无论是在网络相关的应用中实现高吞吐量\[38, 50, 52\]，还是使语言运行环境更加高效\[20, 23, 47, 65\]，或者提供高效的容器环境\[62, 76\]，都可以举出一些例子。即使在硬件领域，特别是随着摩尔定律的消亡，制造商越来越倾向于硬件专化用，以实现越来越好的性能；这在机器学习领域尤为明显\[30, 32, 34\]。

> Specialization is arguably the most effective way to achieve outstanding performance, whether it is for achieving high throughput in network-bound applications \[38, 50, 52\], making language runtime environments more efficient \[20, 23, 47, 65\], or providing efficient container environments \[62, 76\], to give some examples. Even in the hardware domain, and especially with the demise of Moore’s law, manufacturers are increasingly leaning towards hardware specialization to achieve ever better performance; the machine learning field is a primary exponent of this \[30, 32, 34\].

在虚拟化领域，幺内核是专用化的黄金标准，在吞吐量、内存消耗和启动时间等方面显示出令人印象深刻的结果\[36, 40, 45, 47, 48\]。其中一些好处来自于拥有单一的内存地址空间，从而消除了昂贵的系统调用开销，但其中许多是能够在正确的抽象层次上绑定应用程序以获得最佳性能的结果：例如，一个旨在为每秒数百万个请求提供服务的网络服务器可以访问一个低层次的、基于批处理的网络 API，而不是标准但缓慢的套接字 API。这样的方法已经在一些幺内核项目中被采用，但往往是以一种临时的、即用即弃的方式\[38, 48, 52\]。总的来说，尽管有明显的好处，但幺内核仍有两个主要的缺点：

> In the virtualization domain, unikernels are the golden standard for specialization, showing impressive results in terms of throughput, memory consumption, and boot times, among others \[36, 40, 45, 47, 48\]. Some of those benefits come from having a single memory address space, thus eliminating costly syscall overheads, but many of those are the result of being able to hook the application at the right level of abstraction to extract best performance: for example, a web server aiming to service millions of requests per second can access a low-level, batch-based network API rather than the standard but slow socket API. Such an approach has been taken in several unikernel projects but often in an ad hoc, build-and-discard manner \[38, 48, 52\]. In all, despite clear benefits, unikernels suffer from two major drawbacks:

- 它们需要大量的专业工作来构建并优化出高性能；在大多数情况下，这种工作必须为每个目标应用重新进行。
- 它们通常不符合 POSIX 标准，需要移植应用程序和语言环境。

> - They require significant expert work to build and to extract high performance; such work has to, for the most part, be redone for each target application.
> - They are often non-POSIX compliant, requiring porting of applications and language environments.

我们认为这些缺点并不是根本性的，并提出了一个专门为解决这些问题而建立的幺内核架构。现有的幺内核项目，甚至是那些基于库架构的项目，往往是由小型但单一的内核组成，这些内核的组件具有复杂的、交织在一起的、有时不透明的 API。这意味着开发者不仅要经常将应用程序移植到这样的系统中，而且优化它们的性能需要深入研究代码和（幺）内核的具体情况，以了解如何最好地获得性能提升。

> We argue that these drawbacks are not fundamental, and propose a unikernel architecture built specifically to address them. Existing unikernel projects, even those based on library architectures, tend to consist of small but monolithic kernels that have complex, intertwined and sometimes opaque APIs for their components. This means that developers not only have to often port applications to such systems, but that optimizing their performance requires digging into the code and the specifics of the (uni)kernel in order to understand how to best obtain performance gains.

此外，这类系统通常依赖于面向尺寸的专用化：去除所有不必要的组件以实现最小的镜像。虽然这种策略已经提供了很大的好处，但我们认为，基于库架构的幺内核应该便于获得真正的专用化，允许用户为特定的应用、环境约束和关键性能指标选择最佳的系统组件。

> Further, such systems typically rely on size-based specialization: removing all unnecessary components to achieve minimal images. While this strategy already offers significant benefits, we argue that unikernels based on library architectures should ease access to true specialization, allowing users to choose the best system component for a given application, environmental constraints, and key performance indicators.

在本文中，我们提出了 Unikraft，一个新颖的微库操作系统，旨在无痛、无缝地生成专用化、高性能的幺内核。要做到这一点，Unikraft 依赖于两个关键原则：

> In this paper we propose Unikraft, a novel micro-library operating system targeted at painlessly and seamlessly generating specialized, high performance unikernels. To do so, Unikraft relies on two key principles:

- 内核应该是完全模块化的，以便使幺内核能够完全和容易地定制。在 Unikraft 中，诸如内存分配器、调度器、网络堆栈和早期启动代码等操作系统基元是独立的微库。
- 内核应该提供注重性能的、定义明确的 API，这些 API 可以很容易地被选择和组成，以满足应用程序的性能需求。在 Unikraft 中，这样的 API 本身就是微库，这意味着它们可以很容易地被加入或从构建中移除，而且它们的功能可以通过提供额外的这种微库来扩展。

> - The kernel should be fully modular in order to allow for the unikernel to be fully and easily customizable. In Unikraft, OS primitives such as memory allocators, schedulers, network stacks and early boot code are stand-alone micro-libraries.
> - The kernel should provide performance-minded, well-defined APIs that can be easily selected and composed in order to meet an application’s performance needs. In Unikraft, such APIs are micro-libraries themselves, meaning that they can be easily added to or removed from a build, and that their functionality can be extended by providing additional such micro-libraries.

简而言之，Unikraft 的关键概念创新是为操作系统核心组件定义了一套小的 API，当不需要的时候可以很容易地替换掉一个组件，面对不同性能需求可以从同一个组件的多个实现中挑选。构建 API 时考虑了性能（例如，通过设计支持批处理）和最小化（没有不需要的功能）。

> In brief, the key conceptual innovation of Unikraft is defining a small set of APIs for core OS components that makes it easy to replace-out a component when it is not needed, and to pick-and-choose from multiple implementations of the same component when performance dictates. The APIs have been built with performance (e.g., by supporting batching by design) and minimality in mind (no unneeded features).

为了支持广泛的应用，我们移植了 musl libc 库，并提供了一个系统调用 shim 层微库。因此，在 Unikraft 上运行一个应用程序可以像用其本地构建系统构建它一样简单，并将产生的对象文件链接回 Unikraft。此外，Unikraft 支持许多已经移植的应用程序（如 SQLite、nginx、Redis）、编程语言和运行环境，如 C/C++、Go、Python、Ruby、Web Assembly 和 Lua，以及一些不同的 Hypervisor/VMM（截至本文写作时，QEMU/KVM、Xen、Firecracker\[4\] 和 Solo5\[78\]）。

> To support a wide range of applications, we port the musl libc library, and provide a syscall shim layer micro-library. As a result, running an application on Unikraft can be as simple as building it with its native build system, and linking the resulting object files back into Unikraft. In addition, Unikraft supports a number of already-ported applications (e.g., SQLite, nginx, Redis), programming languages and runtime environments such as C/C++, Go, Python, Ruby, Web Assembly and Lua, and a number of different hypervisors/VMMs (QEMU/KVM, Xen, Firecracker \[4\], and Solo5 \[78\] as of this writing).

我们在 Unikraft 上使用此类应用的评估结果是，与 Linux 虚拟机相比，性能提高了 1.7-2.7 倍。此外，这些应用程序的 Unikraft 镜像约为 1MB，需要不到 10MB 内存来运行，并且在 VMM 时间之外，可以在 1ms 内启动（总启动时间 2-40 ms）。Unikraft 是 Linux 基金会的一个开源项目，其源代码可以在 <https://www.unikraft.org> 找到。

> Our evaluation using such applications on Unikraft results in a 1.7x-2.7x performance improvement compared to Linux guests. In addition, Unikraft images for these apps are around 1MB, require less than 10MB of RAM to run, and boot in around 1ms on top of the VMM time (total boot time 2ms-40ms). Unikraft is a Linux Foundation open source project and the sources can be found at www.unikraft.org.

## 2 设计理念和方案空间

> 2 Design Principles and Solution Space

在得出 Unikraft 的关键设计原则之前，值得分析一下传统操作系统中那些不必要的或不适合于单一应用案例的功能和（重量级）机制：

> Before deriving what the key design principles for Unikraft are, it is worth analyzing the features and (heavyweight) mechanisms of traditional OSes that are unnecessary or illsuited to single application use cases:

- 应用程序和内核之间的保护域切换在虚拟化环境下可能是多余的，因为隔离是由 Hypervisor 程序保证的，而且会导致可观的性能下降。
- 多个地址空间在单个应用域中可能是无用的，但在标准操作系统中取消这种支持需要大量工作来重新实现。
- 对于 RPC 风格的服务器应用，线程是不需要的，一个单一的、运行到完成的事件循环就足以实现高性能。这将消除对虚拟机内的调度器及其相关开销的需求，以及虚拟机和 Hypervisor 调度器之间的不匹配\[19\]。
- 对于以性能为导向的基于 UDP 的应用程序，操作系统网络协议栈的大部分都是无用的：应用程序可以简单地使用驱动 API，就像 DPDK 风格的应用程序已经做的那样。目前还没有办法从标准的操作系统中轻松删除网络协议栈而不是整个网络子系统。
- 应用程序直接访问 NVMe 存储，消除了对文件描述符、VFS 层和文件系统的需求，但从现有的操作系统中移除这种支持，包括围绕着存储 API 的多层结构，是非常困难的。
- 内存分配器对应用程序的性能有很大的影响，而通用分配器已经被证明对许多应用程序来说不是最优的\[66\]。因此，如果每个应用程序可以选择自己的分配器，那将是最理想的；然而，这在今天的操作系统中很难做到，因为内核所使用的分配器是内置的。

> - Protection-domain switches between the application and the kernel might be redundant in a virtualization context because isolation is ensured by the hypervisor, and result in measurable performance degradation.
> - Multiple address spaces may be useless in a single application domain, but removing such support in standard OSes requires a massive reimplementation effort.
> - For RPC-style server applications, threading is not needed, with a single, run-to-completion event loop sufficing for high performance. This would remove the need for a scheduler within the VM and its associated overheads, as well as the mismatch between the guest and hypervisor schedulers \[19\].
> - For performance-oriented UDP-based apps, much of the OS networking stack is useless: the app could simply use the driver API, much like DPDK-style applications already do. There is currently no way to easily remove just the network stack but not the entire network sub-system from standard OSes.
> - Direct access to NVMe storage from apps removes the need for file descriptors, a VFS layer and a filesystem, but removing such support from existing OSes, built around layers of the storage API, is very difficult.
> - Memory allocators have a large impact on application performance, and general purpose allocators have been shown to be suboptimal for many apps \[66\]. It would therefore be ideal if each app could choose its own allocator; this is however very difficult to do in today’s operating systems because the allocators that kernels use are baked in.

这个公认的、不完备的应用专用优化列表意味着，对于标准操作系统提供的每一个核心功能，至少存在一个或几个不需要它的应用。移除这样的功能会减少代码大小和资源使用，但往往需要进行重要的重新设计工作。

> This admittedly non-exhaustive list of application-specific optimizations implies that for each core functionality that a standard OS provides, there exists at least one or a few applications that do not need it. Removing such functionality would reduce code size and resource usage but would often require an important re-engineering effort.

我们想要解决的问题是使开发者能够为每一个应用程序创建一个专门的操作系统，以确保尽可能的最佳性能，同时约束与操作系统相关的开发工作，并使现有的应用程序易于移植。这一分析指出了一些关键的设计决策：

> The problem we want to solve is to enable developers to create a specialized OS for every single application to ensure the best performance possible, while at the same time bounding OS-related development effort and enabling easy porting of existing applications. This analysis points to a number of key design decisions:

- 单一地址空间：以单一应用场景为目标，可能有不同的应用通过网络通信交流。
- 完全模块化的系统：所有组件，包括操作系统基元、驱动程序、平台代码和库，都应该易于根据需要添加和删除；甚至 API 也应该是模块化的。
- 单一的特权级：不应该有用户/内核空间的分离，以避免昂贵的处理器模式切换。这并不排斥区隔化（例如微库），这可以以合理的开销实现\[69\]。
- 静态链接：启用编译器功能，例如死代码消除（DCE）和链接时优化（LTO），以自动摆脱不需要的代码。
- 支持 POSIX：为了支持现有的或遗留的应用程序和编程语言，同时仍允许在该 API 下实现专用化。
- 平台抽象化：为一系列不同的 Hypervisor/VMM 无缝生成镜像。

> - Single address space: Target single application scenarios, with possibly different applications talking to each other through networked communications.
> - Fully modular system: All components, including operating system primitives, drivers, platform code and libraries should be easy to add and remove as needed; even APIs should be modular.
> - Single protection level: There should be no user/kernel-space separation to avoid costly processor mode switches. This does not preclude compartmentalization (e.g., of micro-libraries), which can be achieved at reasonable cost \[69\].
> - Static linking: Enable compiler features, e.g., Dead Code Elimination (DCE) and Link-Time Optimization (LTO), to automatically get rid of unneeded code.
> - POSIX support: In order to support existing or legacy applications and programming languages while still allowing for specialization under that API.
> - Platform abstraction: Seamless generation of images for a range of different hypervisors/VMMs.

有鉴于此，问题在于如何实现这样一个系统：最小化现有的通用操作系统、从现有的幺内核项目开始，或者从头开始。

> Given these, the question is how to implement such a system: by minimizing an existing general-purpose operating system, by starting from an existing unikernel project, or from scratch.

现有的工作在解决这个问题方面有三个方向。第一个方向是采用现有的操作系统并增加或删除功能。主要的例子是增加对单一地址空间的支持和删除保护域的交叉。OSv \[37\]和 Rump \[36\]采用了 BSD 内核的部分内容，并重新设计了它以在幺内核环境下工作；Lupine Linux \[40\]依赖于 Linux 内核的最小化、专用化配置和内核模式 Linux（KML）补丁。这些方法使应用程序的移植变得容易，因为它们提供了二进制兼容性或 POSIX 兼容性，但所产生的内核是宏式的的。

> Existing work has taken three directions in tackling this problem. The first direction takes existing OSes and adds or removes functionality. Key examples add support for a single address space and remove protection domain crossings: OSv \[37\] and Rump \[36\] adopt parts of the BSD kernel and re-engineer it to work in a unikernel context; Lupine Linux \[40\] relies on a minimal, specialized configuration of the Linux kernel with Kernel Mode Linux (KML) patches. These approaches make application porting easy because they provide binary compatibility or POSIX compatibility, but the resulting kernel is monolithic.

现有的宏内核操作系统确实有每个组件的 API，但大多数 API 都相当复杂，因为它们是有机地发展起来的，而且组件的分离往往是模糊的，以实现性能（例如， `sendfile` 在网络和存储堆栈之间建立短路）。例如，由于历史原因，Linux 内核具有高度相互依赖的子系统\[8\]。

> Existing monolithic OSes do have APIs for each component, but most APIs are quite rich as they have evolved organically, and component separation is often blurred to achieve performance (e.g., sendfile short circuits the networking and storage stacks). The Linux kernel, for instance, historically featured highly inter-dependent subsystems \[8\].

为了更好地量化这种 API 的复杂性，我们分析了 Linux 内核的主要组件之间的依赖关系。作为一个粗略的近似，我们使用内核源代码树中的子目录来识别（广泛的）组件。我们使用 cscope 从所有内核组件的源中提取所有的函数调用，然后对每个调用进行检查，看该函数是定义在同一个组件中还是不同的组件中；对于后者，我们记为一次依赖。我们在图 1 中绘制了依赖关系图：边上的注释显示了节点之间的依赖关系的数量。这个密集的图明显表明，移除或替换 Linux 内核中的任何一个组件都需要了解并修复其他组件的所有依赖关系，这是一项艰巨的任务。

> To better quantify this API complexity, we analyzed dependencies between the main components of the Linux kernel. As a rough approximation, we used the subdirectories in the kernel source tree to identify (broad) components. We used cscope to extract all function calls from the sources of all kernel components, and then for each call checked to see if the function is defined in the same component or a different one; in the latter case, we recorded a dependency. We plot the dependency graph in Figure 1: the annotations on the edges show the number of dependencies between nodes. This dense graph makes it obvious that removing or replacing any single component in the Linux kernel requires understanding and fixing all the dependencies of other components, a daunting task.

虽然完全的模块化是困难的，但是 Rump 已经成功地将宏内核的某些部分进行了模块化。在那里，NetBSD 内核被分割成基础层（所有内核都必须使用）、由主机提供的功能（调度、内存分配等）以及可以独立运行的所谓“阵营”（例如，网络或文件系统支持）。Rump 在一定程度上实现了我们的目标，但是仍然有许多依赖性，要求所有内核都有基础层和“超调用层”（hypercall layers）。

> While full modularization is difficult, modularizing certain parts of a monolithic kernel has been done succesfully by Rump. There, the NetBSD kernel was split into base layers (which must be used by all kernels), functions provided by the host (scheduling, memory allocation,etc) and so-called factions that can be run on their own (e.g. network or filesystem support). Rump goes some way towards achieving our goals, however there are still many dependencies left which require that all kernels have the base and hypercall layers.

第二个方向是完全绕过操作系统（OS），主要是为了 I/O 性能，同时保留原有的协议栈--浪费了进程的资源。即便如此，也需要进行移植工作，因为应用程序必须针对新的网络（DPDK、netmap \[64\]或 Linux 的 io_uring \[11\]子系统）或存储（SPDK）API 进行编码。

> The second direction is to bypass the operating system (OS) altogether, mostly for I/O performance, while leaving the original stack in place – wasting resources in the process. Even here, porting effort is required as apps must be coded against the new network (DPDK, netmap \[64\] or Linux’s io_uring \[11\] subsystem) or storage (SPDK) API.

第三个方向是为每个目标应用从头开始添加所需的操作系统功能，可能通过复用现有操作系统的代码。这是 ClickOS \[51\]支持 Click 模块化路由器，MirageOS \[46\]支持 OCaml 应用，以及 MiniCache \[39\]实现网络缓存所采取的方法，仅举几例。由此产生的镜像非常精简，有很好的性能，而且启动时间小；最大的问题是，移植的工作量很大，而且大多要为每一个应用程序或语言重复进行。

> The third direction is to add the required OS functionality from scratch for each target application, possibly by reusing code from existing operating systems. This is the approach taken by ClickOS \[51\] to support Click modular routers, MirageOS \[46\] to support OCaml applications, and MiniCache \[39\] to implement a web cache, to name a few. The resulting images are very lean, have great performance and have small boot times; the big problem is that the porting effort is huge, and that it has to be mostly repeated for every single application or language.

总而言之，从现有的项目开始不是最好的选择，因为上述三个方向的项目都不是为了支持我们所概述的关键原则而设计的。我们选择从头开始设计 API，尽管我们在相关的地方重用了现有作品的组件。

> In sum, starting from an existing project is suboptimal since none of the projects in the three directions mentioned were designed to support the key principles we have outlined. We opt for a clean-slate API design approach, though we do reuse components from existing works where relevant.

## 3 Unikraft 架构和 API

> 3 Unikraft Architecture and APIs

传统的操作系统可以粗略地分为宏内核（具有很好的性能）以及能很好地隔离操作系统组件的微内核（以牺牲性能为代价）。与这些工作相比，我们的工作
既包括宏式设计（组件之间没有保护）也包括微内核鼓吹的模块化。

> In contrast to classical OS work, which can be roughly split between monolithic kernels (with great performance) versus micro-kernels that provide great isolation between OS components (at the expense of performance), our work embraces both the monolithic design (no protection between components) and the modularity that micro-kernels advocated.

我们使用模块化来实现专用性，将操作系统的功能分割成细粒度的组件，这些组件只在定义明确的 API 边界内进行通信。我们的主要看法是，我们可以通过精心的 API 设计和静态链接来获得性能，而不是通过短路 API 边界来获得性能。为了实现模块化的总体原则，Unikraft 由两个主要部分组成：

> We use modularity to enable specialization, splitting OS functionality into fine-grained components that only communicate across well-defined API boundaries. Our key observation is that we can obtain performance via careful API design and static linking, rather than short-circuiting API boundaries for performance. To achieve the overarching principle of modularity, Unikraft consists of two main components:

- 微库。微库是实现 Unikraft 核心 API 之一的软件组件；我们将它们与库区分开来，因为它们具有最小的依赖性，可以是任意小的，例如一个调度器。所有实现相同 API 的微库都是可以互换的。一个这样的 API 包含多个内存分配器，它们都实现了 `ukalloc` 接口。此外，Unikraft 支持的库可以提供来自外部库项目（OpenSSL、musl、Protobuf \[31\]等）、应用程序（SQLite、Redis 等）、甚至平台（如 Solo5、Firecracker、Raspberry Pi 3）的功能。
- 构建系统。它提供了一个基于 Kconfig 的菜单，让用户选择在应用程序构建中使用哪些微库，让他们选择目标平台和 CPU 架构，甚至在需要时配置单个微库。然后，构建系统对所有的微库进行编译，将它们链接起来，并为每个选定的平台生成一个二进制文件。

> - Micro-libraries: Micro-libraries are software components which implement one of the core Unikraft APIs; we differentiate them from libraries in that they have minimal dependencies and can be arbitrarily small, e.g., a scheduler. All micro-libraries that implement the same API are interchangeable. One such API contains multiple memory allocators that all implement the ukalloc interface. In addition, Unikraft supports libraries that can provide functionality from external library projects (OpenSSL, musl, Protobuf \[31\], etc.), applications (SQLite, Redis, etc.), or even platforms (e.g., Solo5, Firecracker, Raspberry Pi 3).
> - Build system: This provides a Kconfig-based menu for users to select which micro-libraries to use in an application build, for them to select which platform(s) and CPU architectures to target, and even configure individual micro-libraries if desired. The build system then compiles all of the micro-libraries, links them, and produces one binary per selected platform.

图 4 显示了 Unikraft 的架构。所有的组件都是微库，有自己的 Makefile 和 Kconfig 配置文件，因此可以独立地添加到 unikernel 构建中^1^。API 也是微库，可以通过 Kconfig 菜单轻松启用或禁用；因此幺内核可以编排选择哪些 API，以最好地迎合应用程序的需求（例如，一个 RCP 风格的应用程序可能会关闭 `uksched` API，以实现高性能、运行到完成的事件循环）。

> Figure 4 shows Unikraft’s architecture. All components are micro-libraries that have their own Makefile and Kconfig configuration files, and so can be added to the unikernel build independently of each other^1^. APIs are also micro-libraries that can be easily enabled or disabled via a Kconfig menu; unikernels can thus compose which APIs to choose to best cater to an application’s needs (e.g., an RCP-style application might turn off the uksched API in order to implement a high performance, run-to-completion event loop).

---

1. 当然，除非一个微库依赖另一个微库，在这种情况下，构建系统也会构建该依赖。

> 1. Unless, of course, a micro-library has a dependency on another, in which case the build system also builds the dependency.

---

Unikraft 的体系结构还包括增加 POSIX 支持的组件，使其相对容易地支持现有的应用程序（更多内容见第 4 节）。Unikraft 可以通过两种方式提高应用程序的性能：

> Unikraft’s architecture also includes components that add POSIX support, making it relatively easy to support existing applications (more on this in §4). Unikraft can improve the performance of applications in two ways:

1. 未修改的应用程序，通过消除系统调用的开销，减少镜像大小和内存消耗，以及选择高效的内存分配器。
2. 专用化，通过调整应用程序，在性能至关重要的地方利用较低级别的 API（例如，寻求高磁盘 I/O 吞吐量的数据库应用程序）。

> 1. Unmodified applications, by eliminating syscall overheads, reducing image size and memory consumption, and by choosing efficient memory allocators.
> 2. Specialization, by adapting applications to take advantage of lower level APIs wherever performance is critical (e.g., a database application seeking high disk I/O throughput).

作为 Unikraft 模块化的证明，一个最小的 Hello World 配置在 KVM 上产生一个 200KB 大小的镜像，在 Xen 上产生一个 40KB 大小的镜像，只需要平台 booststrapping 代码和 nolibc——一个 Unikraft 专用的 libc 替代品，只提供基本的最小功能集，如 memcpy 和字符串处理。图 3 显示了该镜像中使用的所有库；相比之下，Linux 中（即使是）Hello World 应用也需要整个 Linux 内核。

> As a proof of Unikraft’s modularity, a minimal Hello World configuration yields an image of 200KB in size on KVM and 40KB on Xen, requiring only platform boostrapping code and nolibc, a Unikraft-specific libc replacement that only provides a basic minimal set of functionality such as memcpy and string processing. All the libraries used in this image are shown in Figure 3; in contrast, the entire Linux kernel in Figure 1 is needed for a Hello World app in Linux.

大多数应用程序确实需要更多的功能（见图 2 中的 nginx 镜像）。请注意（1）这个镜像不包括块设备子系统，因为它只使用内存文件系统，（2）所有的组件都比 Linux 对应的组件更小，依赖更少。这些例子展示了 Unikraft 轻松添加和删除组件的能力，包括核心操作系统的组件，使开发人员能够为他们的应用程序创建高效、专用的镜像。

> Most applications do require more functionality (see nginx image in Figure 2). Note how (1) this image does not include a block subsystem since it only uses RamFS, and (2) how all components are smaller and have fewer dependencies than their Linux counterparts. These examples showcase Unikraft’s ability to easily add and remove components, including core OS ones, allowing developers to create efficient, specialized images for their apps.

轻松地将组件换入和换出的能力，以及在不同层次插入应用程序的能力，为应用程序开发人员提供了广泛的优化可能性。首先，未经修改的应用程序（如 Hello World 和 nginx）可以使用musl（图 4 中的 ➀）或 nolibc 的 posix-compatibility 层，由于省略系统调用开销，透明地获得低启动时间、低内存消耗和提高吞吐量，因为 Unikraft 的系统调用实际上就是函数调用。

> The ability to easily swap components in and out, and to plug applications in at different levels presents application developers with a wide range of optimization possibilities. To begin with, unmodified applications (e.g. Hello World and nginx) can use the posix-compatibility layer with musl (➀ in Figure 4) or nolibc, transparently getting low boot times, lower memory consumption and improved throughput because of the lack of syscall overheads, as Unikraft syscalls are effectively function calls.

同样，应用程序的开发者可以很容易地选择一个合适的内存分配器（➅）来获得最大的性能，或者在同一个幺内核中使用多个不同的内存分配器（例如，一个简单、快速的内存分配器用于启动代码，而一个标准的内存分配器用于应用程序本身）。

> Likewise, the application developer can easily select an appropriate memory allocator (➅) to obtain maximum performance, or to use multiple different ones within the same unikernel (e.g., a simple, fast memory allocator for the boot code, and a standard one for the application itself).

对快速启动感兴趣的开发者可以通过提供他们自己的启动代码（➄）来遵守 `ukboot` 的 API 来进一步优化幺内核；在第 6 节中，我们展示了两个启动代码微库的实验，一个是静态内存页，一个是动态内存页，显示了启动时间和内存分配灵活性之间的折衷。

> Developers interested in fast boot times could further optimize the unikernel by providing their own boot code (➄) to comply with the ukboot API; in §6 we show experiments with two boot code micro-libraries, one with static memory pages and one with dynamic ones, showing the trade-off between boot time and memory allocation flexibility.

对于网络相关的应用，开发者可以使用标准的套接字接口（➁）或更低级、更高性能的 `uknetdev` API（➆），以显著提高吞吐量；我们将在下面更详细地讨论这个 API，并将在第 6 节评估它。同样地，数据库等受磁盘约束的应用可以通过 `vfscore` 微库（➂）遵循标准路径，或者通过对 `ukblock` API（➇）进行编码来优化吞吐量。调度器也是可插拔的（➃），每个 CPU 核可以运行不同的调度器（即使多核支持尚未完成）。

> For network-bound applications, the developers can use the standard socket interface (➁) or the lower level, higher performance uknetdev API (➆) in order to significantly improve throughput; we will discuss this API in greater detail below, and will evaluate it in §6. Similarly, disk-bound applications such as databases can follow a standard path through the vfscore micro-library (➂), or optimize throughput by coding against the ukblock API (➇). Schedulers are also pluggable (➃), and each CPU core can run a different scheduler (even if multi-core support is still work in progress).

我们将在本文后面介绍和评估其中的几个场景，但首先我们通过关注其中的一个子集，对 Unikraft 的 API 进行更深入的研究。

> We will visit and evaluate several of these scenarios later on in the paper, but first we give a more in-depth look into Unikraft’s APIs by focusing on a subset of them.

### 3.1 `uknetdev` API

> 3.1 uknetdev API

Unikraft 的网络子系统将设备驱动端（如 virtio-net、netfront）与网络堆栈或低级网络应用（简称应用）解耦。

> Unikraft’s networking sub-system decouples the device driver side (e.g., virtio-net, netfront) from the network stack or low-level networking application (application for short).

关于前者，简单地替换网络协议栈在商业操作系统中并不常见；相反，驱动程序通常是为特定的网络协议栈实现的。这个 API 的目的是将这两个组件解耦，以使驱动程序能够跨平台重用。

> Regarding the former, easily swapping network stacks is something that is not common in commodity OSes; instead, drivers are usually implemented for a particular network stack. The aim of this API is to decouple these two components in order to allow drivers to be reused across platforms.

对于后者，一个网络应用或网络协议栈应该能够在不同的平台上未经修改地运行，并对接不同的驱动程序。因为我们正在处理大量的使用案例，API 不应该限制任何一个案例，也不应该成为高性能工作负载的潜在性能瓶颈。我们从英特尔 DPDK 的 `rte_netdev` API 中获得了部分设计。然而，由于它的重点是高性能而不是高效的资源使用，我们设计了一个 API，允许应用程序在轮询、中断驱动或混合模式下操作 Unikraft 驱动程序。

> For the latter, a networking application or network stack should be able to run unmodified on a different platform with different drivers. Because we are addressing a wide range of use cases, the API should not restrict any of them nor become a potential performance bottleneck for high performance workloads. We derived part of the design from Intel DPDK’s rte_netdev API. However, because its focus is on high performance rather than efficient resource usage, we designed an API that allows applications to operate Unikraft drivers in polling, interrupt-driven, or mixed mode.

此外，`uknetdev` 将内存管理留给应用程序，同时支持高性能特性，如多队列、零拷贝 I/O 和数据包批处理。我们让应用程序完全操作和初始化驱动程序；驱动程序不会自己运行任何初始化程序。相反，我们为应用程序提供 API 接口，以提供必要的信息（例如，支持的队列数量和卸载功能），这样，应用程序代码可以通过挑选最佳的驱动程序属性和功能集来实现专用化。驱动程序将其回调（例如，发送和接收）注册到 `uk_netdev` 结构中，然后应用程序使用该结构来调用驱动程序的例程。

> In addition, uknetdev leaves memory management to the application, all the while supporting high performance features like multiple queues, zero-copy I/O, and packet batching. We let the application fully operate and initialize the driver; drivers do not run any initialization routine on their own. Instead, we provide API interfaces for applications to provide necessary information (e.g., supported number of queues and offloading features) so that the application code can specialize by picking the best set of driver properties and features. Drivers register their callbacks (e.g, send and receive) to a uk_netdev structure which the application then uses to call the driver routines.

为了开发独立于应用程序的网络驱动程序，同时使用应用程序或网络协议栈的内存管理，我们引入了一个称为 `uk_netbuf` 的网络包缓冲器包装结构。这个结构包含了驱动程序在数据包缓冲区中发送或接收数据所需的元信息；我们的想法是，应用程序使用这个结构来分配数据包缓冲区，但布局是由应用程序控制的。由于驱动程序和 API 都不管理分配，所以性能关键的工作负载可以利用预先分配的网络缓冲区池，而内存效率高的应用程序可以通过从标准堆分配缓冲区来减少内存占用。API 的发送和接收调用看起来如下。

> In order to develop application-independent network drivers while using the application’s or network stack’s memory management we introduce a network packet buffer wrapper structure called uk_netbuf. This struct contains meta information needed by the driver to send or receive data in the packet buffer; the idea is that applications use this structure for packet buffer allocations, but the layout is under the control of the application. Since neither the driver nor the API manage allocations, performance critical workloads can make use of pre-allocated network buffers pools, while memory efficient applications can reduce memory the footprint by allocating buffers from the standard heap. The send and receive calls of the API look as follows:

```c
int uk_netdev_tx_burst(struct uk_netdev *dev,
                       uint16_t queue_id,
                       struct uk_netbuf **pkt,
                       __u16 *cnt);
int uk_netdev_rx_burst(struct uk_netdev *dev,
                       uint16_t queue_id,
                       struct uk_netbuf **pkt,
                       __u16 *cnt);
```

用户交出 `uk_netbufs` 的数组并指定其长度。在发送时，驱动程序从给定的数组中尽可能多地排队等待 `netbufs`（大小是用 `cnt` 给定的）。该函数返回标志，表明队列中是否还有空间来发送更多的数据包，或者队列是否已满。`cnt` 参数也被用作输出参数，以表明有多少数据包被实际放在发送队列中。接收功能以类似的方式工作。

> The user hands over arrays of uk_netbufs and specifies their length. On transmit, the driver enqueues as many netbufs as possible from the given array (size is given with cnt). The function returns flags that indicate if there is still room on the queue to send more packets or if the queue is full. The cnt parameter is also used as an output parameter to indicate how many packets were actually placed on the send queue. The receive function works in a similar manner.

默认情况下，驱动程序以轮询模式操作队列，但 API 还有一个接口可以为特定队列启用中断模式。在这种模式下，每当函数表明没有更多的工作要做（没有更多的数据包接收或发送队列已满），队列的中断线就被启用。在驱动配置过程中，应用程序可以为每个队列注册一个中断处理程序，一旦收到数据包或发送队列的空间变得可用，该程序就被调用。之后，中断线不活动，直到发送或接收功能根据队列状态再次激活它，例如，当应用程序收到所有数据包时。数据包如何以及在哪里被接收或传输完全取决于应用程序的实现。例如，中断回调可以用来解除对接收或发送线程的封锁，但也可以包含在事件循环的实现中。一旦中断到来，应用程序就知道接收或发送函数必须被调用。这种实现方式避免了中断风暴，并能在重负载情况下自动过渡到轮询模式。

> As default, a driver operates a queue in polling mode, but the API has one more interface to enable interrupt mode for a specific queue. In this mode, whenever the function indicates that there is no more work to be done (no more packets received or the send queue is full), the interrupt line of the queue is enabled. During driver configuration the application can register an interrupt handler per queue which is called as soon as a packet is received or space becomes available on the transmit queue. Afterwards, the interrupt line is inactive until the transmit or receive function activates it again according to the queue state, for instance when all packets were received by the application. How and where packets are received or transmitted is entirely up to the application’s implementation. For instance, the interrupt callback could be used to unblock a receiving or sending thread, but could also be included into the eventloop implementations. As soon as an interrupt arrives the application knows that the receive or send function has to be called. This implementation avoids interrupt storms and enables automatic transition to polling mode under heavy load situations.

### 3.2 `ukalloc` API

> 3.2 ukalloc API

Unikraft 的内存分配子系统由三层组成：（1）一个符合 POSIX 标准的外部 API，（2）一个叫做 `ukalloc` 的内部分配 API，以及（3）一个或多个后端分配器实现。外部接口的动机是向后兼容，以方便将现有的应用程序移植到 Unikraft。在 C 语言的情况下，外部 API 由一个修改过的标准库暴露出来，这个标准库可以是 nolibc（一个最小的、针对 Unikraft 的 libc 实现）、newlib 或 musl。外部分配接口作为 Unikraft 专用的内部分配接口的兼容包装，它反过来将分配请求重定向到适当的分配器后端（每个分配器有它自己的、独立的内存区域）。因此，内部分配接口作为一个复用设施，使得在同一个幺内核中存在多个内存分配后端。

> Unikraft’s memory allocation subsystem is composed of three layers: (1) a POSIX compliant external API, (2) an internal allocation API called ukalloc, and (3) one or more backend allocator implementations. The external interface is motivated by backward compatibility to facilitate the porting of existing applications to Unikraft. In the case of the C language, the external API is exposed by a modified standard library which can be nolibc (a minimal, Unikraft-specific libc implementation), newlib or musl. The external allocation interface acts as a compatibility wrapper for the Unikraft-specific internal allocation interface, which in turn redirects allocation requests to the appropriate allocator backend (each allocator has its own, separate memory region). The internal allocation interface therefore serves as a multiplexing facility that enables the presence of multiple memory allocation backends within the same unikernel.

Unikraft 的分配接口暴露了 POSIX 接口的 `uk_` 前缀版本：`uk_malloc()`、`uk_calloc()` 等。与 POSIX 不同的是，这些函数要求调用者指定应该使用哪个分配后端来满足请求。`uk_malloc()` 的定义如下：

> Unikraft’s allocation interface exposes uk_ prefixed versions of the POSIX interface: uk_malloc(), uk_calloc(), etc. In contrast to POSIX, these functions require the caller to specify which allocation backend should be used to satisfy the request. uk_malloc() is defined as:

```c
static inline void *
uk_malloc (struct uk_alloc *a, size_t size);
```

`struct uk_alloc *` 参数代表分配后端。这个结构包含指向分配器实现 POSIX 分配接口的函数指针：`malloc()`、`calloc()`、`posix_memalign()` 等。请注意，`uk_malloc()`，像大多数内部分配接口一样，被设计成内联方法，以避免分配调用链中的任何额外的函数调用开销。

> The struct uk_alloc *argument represents the allocation backend. This structure contains function pointers that refer to the allocator’s implementation of the POSIX allocation interface: malloc(), calloc(), posix_memalign(), etc. Note that uk_malloc(), like most of the internal allocation interface, is designed as an inline method in order to avoid any additional function call overhead in the allocation path.

分配器必须指定一个初始化函数，由 `ukboot` 在启动过程的早期阶段调用。初始化函数被传递给一个指向堆的第一个可用字节的 `void*` 裸指针，以及一个指定堆大小的 `size_t len` 参数。他们必须完全初始化分配器，并在 `ukalloc` 接口注册分配器。一旦初始化函数返回，分配器就被认为可以满足内存分配的要求。启动过程设置了内存分配器和内存源之间的关联。

> Allocators must specify an initialization function which is called by ukboot at an early stage of the boot process. Initialization functions are passed a void* base pointer to the first usable byte of the heap, along with a size_t len argument which specifies the size of the heap. They must fully initialize the allocator and register the allocator with the ukalloc interface. The allocator is considered ready to satisfy memory allocations as soon as the initialization function returns. The boot process sets the association between memory allocators and memory sources.

Unikraft 支持五种分配后端：伙伴系统、两级隔离的 Fits \[53\]（TLSF）实时内存分配器、tinyalloc \[67\]、Mimalloc \[42\]（1.6.1版本）和 Oscar \[12\] 安全内存分配器。一个特殊的情况是垃圾收集（GC）内存分配器，需要一个线程来执行 GC。我们可以用两个分配器来实现这些分配器，一个用于早期启动时间，初始化 GC 线程，然后是主 GC 分配器，在其线程启动后立即接管；我们对 Mimalloc 使用这种解决方案，因为它有一个 pthread 的依赖。

> Unikraft supports five allocation backends: a buddy system, the Two-Level Segregated Fits [53] (TLSF) real-time memory allocator, tinyalloc [67], Mimalloc [42] (version 1.6.1) and the Oscar [12] secure memory allocator. A special case are garbage-collection (GC) memory allocators that require a thread to perform GC. We can implement these with two allocators, one for the early boot time that initializes the GC thread, and then the main GC allocator, which takes over as soon as its thread is started; we use this solution for Mimalloc because it has a pthread dependency.

### 3.3 `uksched` 和 `uklock` API

> 3.3 uksched and uklock APIs

与许多操作系统不同，Unikraft 中的调度是可有可无的；这使得构建轻量级单线程的幺内核或运行到完成的幺内核成为可能，避免了由负载内的调度器引起的抖动。示例的用例是提供作为虚拟机（如驱动域）或虚拟网络功能（VNF）的支持功能。

> Unlike many OSes, scheduling in Unikraft is available but optional; this enables building lightweight single-threaded unikernels or run-to-completion unikernels, avoiding the jitter caused by a scheduler within the guest. Example use cases are to provide support functions as virtual machines (as in driver domains), or Virtual Network Functions (VNFs).

与 `ukalloc` 类似，`uksched` 抽象了实际的调度器接口。平台库只提供了基本的机制，如上下文切换和计时器，因此调度逻辑和算法是通过实际的调度器库来实现的（截至目前，Unikraft 支持协作式和抢占式调度器）。

> Similar to ukalloc, uksched abstracts actual scheduler interfaces. The platform library provides only basic mechanisms like context switching and timers so that scheduling logic and algorithms are implemented with an actual scheduler library (Unikraft supports co-operative and pre-emptive schedulers as of this writing).

此外，Unikraft 支持实例化多个调度器，例如，每个可用的虚拟 CPU 或可用 CPU 的子集有一个调度器。以 VNF 为例，由于性能和延迟的原因，人们可以在运行数据面处理的虚拟核心上选择无调度或合作调度，但为控制面选择一个共同的抢占式调度器。

> Additionally, Unikraft supports instantiating multiple schedulers, for example one per available virtual CPU or for a subset of available CPUs. For VNFs for example, one may select no scheduling or cooperative scheduling on virtual cores that run the data plane processing because of performance and delay reasons, but select a common preemptive scheduler for the control plane.

`uklock` 库提供了同步原语，如互斥锁和信号量。为了保持其他库的代码可移植性，`uklock` 会根据幺内核的配置方式选择目标实现。这两个方面是线程和多核支持。在最简单的情况下（无线程和单核），一些原语可以在编译时被完全移除，因为不需要互斥机制。如果启用了多核（我们还不支持），一些原语将使用自旋锁和 RCU，因此在这种情况下它们会被编译进去。

> The uklock library provides synchronization primitives such as mutexes and semaphores. In order to keep the code of other libraries portable, uklock selects a target implementation depending on how the unikernel is configured. The two dimensions are threading and multi-core support. In the simplest case (no threading and single core), some of the primitives can be completely compiled out since there is no need for mutual exclusion mechanisms. If multi-core were enabled (we do not yet support this), some primitives would use spin-locks and RCUs, and so in this case they would be compiled in.
