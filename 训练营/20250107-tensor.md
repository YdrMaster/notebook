# 张量

可能图灵或者乔姆斯基这些上一代的大师会想象包含世界知识的高度复杂的数据也应该具有高度复杂的结构。然而现在的大规模深度神经网络的数据在人类看来却恰恰相反，是高度同质，高度非结构化的。当然这种非结构化实际上并不是没有结构，而是人类的理性思维难以感知这种结构。我们这里就不谈那些理论，从结果上来看，张量就是对这种非结构化数据的一种有效的表示。

从开发者的视角看，张量就是高维数组在深度神经网络程序中的别名。例如 C 语言中这样表示一个数组：

```c
T arr[m][n];
```

读作“m 行 n 列的 T 数组 arr”，或者“m 乘 n 的二维数组 arr”。从这个声明中，我们可以看到这个数组最基本的信息，数据类型 T，形状 `m x n`，维数 2。同时我们可以意识到当我们读写这个数组时，我们实际上读写的是一个 `m x n x sizeof(T)` 的连续存储区域。

> **3x4 数组**
>
> ```plaintext
> \------------/
> | 1  2  3  4 |
> | 5  6  7  8 |
> | 9 10 11 12 |
> /------------\
> ```

在深度神经网络中，最基本的张量定义就是将这个数组动态化起来的一种数据结构。

C++ 表示：

```c++
struct Tensor {
    DataType data_type;        // 数据类型
    std::vector<size_t> shape; // 形状
    std::vector<uint8_t> data; // 底层数据存储
};
```

Rust 表示：

```rust
struct Tensor {
    data_type: DataType, // 数据类型
    shape: Vec<usize>,   // 形状
    data: Vec<u8>,       // 底层数据存储
}
```

事实上，有不少真实可用的深度神经网络程序里真是这样定义张量的，比如 [RefactorGraph](https://github.com/InfiniTensor/RefactorGraph/blob/master/src/04kernel/include/kernel/tensor.h)。可以看到这个张量定义的主体就是数据类型、形状和数据。

在深度神经网络程序中，大部分数据实际上会直接统一到这样的张量定义：

比如：

- 数字，作为变量可能称为“代数类型变量”，在深度学习程序中称为“标量”，表示为一个形状为空——或者说阶数为 0——的张量；

  > BTW，高维数组的维数在张量中称为“阶数”（`rank`）。

  ```plaintext
  shape: []
  ---------
  x
  ```

- 数组，在深度学习程序中称为“向量”，表示为一个阶数为 1 的张量；

  ```plaintext
  shape: [x] = 6
  --------------
  [1, 2, 3, 4, 5, 6]
  ```

- 二维数组，在深度学习程序中称为“矩阵”，表示为一个阶数为 2 的张量；

  ```plaintext
  shape: [y x] = 3 x 4
  --------------------
  \------------/
  | 1  2  3  4 |
  | 5  6  7  8 |
  | 9 10 11 12 |
  /------------\
  ```

- 3 阶到 5 阶的张量在深度神经网络程序中都很常见，尤其 4 阶张量在卷积神经网络中非常常见，以至于英伟达提供的 GPU 算子库 cuDNN 中直接为 4 阶张量的初始化提供了[专门的 API](https://docs.nvidia.com/deeplearning/cudnn/latest/api/cudnn-ops-library.html#cudnnsettensor4ddescriptor)；

  ```plaintext
  shape: [z y x] = 2 x 3 x 4
  --------------------------
  / z
  *--x ____________
  |   /          /|
  y  / 13 14 15 16|
    /----------/20| ...
    |1  2  3  4|24|
    |5  6  7  8| /
    |9 10 11 12|/
    /----------/
  ```

- 更大阶数的张量也偶尔出现；

这个就是张量最基本的定义，到这里后面大家学习模型结构或者看一些简单的神经网络程序应该就没有障碍了。这里特别推荐大家看一下 K 佬的 [llama2.c](https://github.com/karpathy/llama2.c)，现在我们的 InfiniLM 就是在这个项目的基础上做的。看不惯 C 语言的话也可以看我以前照着写的 [rust](https://github.com/YdrMaster/llama2.rs)。

---

但是，InfiniLM 里的张量定义更复杂一些，更复杂的定义来源于深度网络程序的现实需求。张量的形状对于许多计算来说是具有语义的，这种语义会决定计算的方式。例如大家都学过矩阵乘法：

```plaintext
(M x N) <- (M x K) x (K x N)
```

这里出现的 3 个 2 阶张量它们各自的阶数和形状就决定了这个矩阵乘法的计算方式。但是对于基本的张量定义来说，形状和存储方式是一一对应的，所以为了得到需要的张量形状而对张量进行变换的时候就必须同时变换底层的数据表示。由于张量经常在很简单的形状下管理着海量数据，例如作为线性层权重出现的 4096x4096 的 f16 数组，对这样的矩阵转置意味着按每 2 字节一次，挪动 32MiB 字节，对于串行程序来说意味着 3355'4432 次寻址访问，这是非常低效的。

为了解决这个问题，需要把张量的逻辑形状和存储方式分离，变换时尽量只改变逻辑形状，只有在必要的时候才变换存储方式。我们采用的是一种经典的方式，就是为张量引入步长（`strides`）字段：

```rust
struct Tensor {
    data_type: DataType,
    shape: Vec<usize>,
    strides: Vec<isize>,
    data: Vec<u8>,
}
```

步长定义在张量的每一个维度上，表示这一个维度上第 i 个数据和第 i+1 个数据在底层存储上的距离。例如一个 C 二维数组：

```c
T arr[m][n];
```

它的步长就是 `(n, 1)`，因为在索引在长度为 `m` 的这个维度上每增加 1，就意味着数据的地址跳过了 `n` 个元素，用矩阵的语言描述就是矩阵的两行之间隔着一行的长度——也就是列数——那么多个元素。而索引在长度为 `n` 的这个维度上每增加 1，地址也只增加了 1 个元素。这就是所谓行优先存储的矩阵。

所以对于 3x4 的二阶张量（矩阵），步长为 `[4,1]`：

```plaintext
shape:   [     y  x] = 3 x 4
strides: [(n) sy sx] = (12) 4 1
-------------------------------
\------------/    \----------/
| 1  2  3  4 |    |  O → +1  |
| 5  6  7  8 | -> |  ↓       |
| 9 10 11 12 |    | +4       |
/------------\    /----------\
```

对于 2x3x4 的三阶张量（可能是卷积核），步长为 `[12,4,1]`：

```plaintext
shape:   [     z  y  x] = 2 x 3 x 4
strides: [(n) sz sy sx] = (24) 12 4 1
-------------------------------------
/ z
*--x ____________       ___________
|   /          /|      /         /|
y  / 13 14 15 16|     /  +12    / |
  /----------/20|    /--/------/  |
  |1  2  3  4|24| -> | O → +1  |  /
  |5  6  7  8| /     | ↓       | /
  |9 10 11 12|/      |+4       |/
  /----------/       /---------/
```

如果按模型结构，某个矩阵运算需要转置矩阵，对于基本的张量表示来说真的需要挪动矩阵中所有的数据。而引入步长之后我只需要直接把形状和步长的 4 个数字交换一下就完成了：

|     | 原矩阵 | 转置矩阵
| :-: | :-: | :-:
| 形状 | M x N | N x M
| 步长 | N \| 1 | 1 \| N

如果认为这个修改算是优化加速的话，加速比就达到 (16MiB+2) / 4 = 419'4304.5 倍。

---

InfiniLM 使用的[张量定义](https://github.com/InfiniTensor/InfiniLM/blob/main/tensor/src/lib.rs#L17) 是这样的：

```rust
#[derive(Clone)]
pub struct Tensor<T> {
    dt: DigitLayout,
    layout: ArrayLayout<5>,
    physical: T,
}
```

其中对张量布局（包括形状、步长和偏移）的表示封装到了 `ArrayLayout` 类型。对这个类型的定义和变换已经发布到了 [crates.io](https://crates.io/crates/ndarray-layout)。感兴趣的同学可以去阅读这个源码。在 InfiniLM 通过对张量的进一步封装，可以实现张量数据的[格式化](https://github.com/InfiniTensor/InfiniLM/blob/main/tensor/src/fmt.rs)。通过这个功能可以方便地学习理解张量的各种布局变换的功能和效果。
