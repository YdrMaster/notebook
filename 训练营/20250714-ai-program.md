# 人工智能程序的基本表示

## 张量

> 本文合并了[2024 冬张量讲义](20250107-tensor.md)和[2024 冬 AI 程序表示讲义](20250714-ai-program.md)并进行了修订。

可能图灵或者乔姆斯基这些上一代的大师会想象包含世界知识的高度复杂的数据也应该具有高度复杂的结构。然而现在的大规模深度神经网络的数据在人类看来却恰恰相反，是高度同质，高度非结构化的。当然这种非结构化实际上并不是没有结构，而是人类的理性思维难以感知这种结构。从结果上来看，张量就是对这种非结构化数据的一种有效的表示。

从开发者的视角看，张量就是高维数组在深度神经网络程序中的别名。例如 C 语言中这样表示一个数组：

```c
T arr[m][n];
```

读作“m 行 n 列的 T 数组 arr”，或者“m 乘 n 的二维数组 arr”。从这个声明中，我们可以看到这个数组最基本的信息，数据类型 T，形状 `m x n`，维数 2。同时我们可以意识到当我们读写这个数组时，我们实际上读写的是一个 `m x n x sizeof(T)` 的连续存储区域。

> **3x4 数组**
>
> ```plaintext
> \------------/
> | 1  2  3  4 |
> | 5  6  7  8 |
> | 9 10 11 12 |
> /------------\
> ```

在深度神经网络中，最基本的张量定义就是将这个数组动态化起来的一种数据结构。

> 所谓“动态化”，指的是将 `T arr[m][n];` 这样定义的数组中使用关键字和语法现象表达的语义转换为使用字段和值表达。开发编译器和 AI 编译器经常需要决策语义是动态表达还是静态表达。

C++ 表示：

```c++
struct Tensor {
    DataType data_type;        // 数据类型
    std::vector<size_t> shape; // 形状
    std::vector<uint8_t> data; // 底层数据存储（擦除数据类型）
};
```

Rust 表示：

```rust
struct Tensor {
    data_type: DataType, // 数据类型
    shape: Vec<usize>,   // 形状
    data: Vec<u8>,       // 底层数据存储（擦除数据类型）
}
```

事实上，有不少真实可用的深度神经网络程序里就是这样定义张量的，比如 [RefactorGraph](https://github.com/InfiniTensor/RefactorGraph/blob/master/src/04kernel/include/kernel/tensor.h)。可以看到这个张量定义的主体就是数据类型、形状和数据。

在深度神经网络程序中，张量是“数据操作的基本结构”，也就是说，所有数据都会在形式上统一到张量表示。例如：

- 数字，作为变量可能称为“代数类型变量”，在深度学习程序中称为“标量”，表示为一个形状为空——或者说阶数为 0——的张量；

  > - 高维数组的维数在张量中称为“阶数”（`rank`）；
  > - 类似于 0! = 1，0 阶张量被人为定义为标量，即一个数字；

  ```plaintext
  shape: []
  ---------
  x
  ```

- 数组，在深度学习程序中称为“向量”，表示为一个阶数为 1 的张量；

  ```plaintext
  shape: [x] = 6
  --------------
  [1, 2, 3, 4, 5, 6]
  ```

- 二维数组，在深度学习程序中称为“矩阵”，表示为一个阶数为 2 的张量；

  ```plaintext
  shape: [y x] = 3 x 4
  --------------------
  \------------/
  | 1  2  3  4 |
  | 5  6  7  8 |
  | 9 10 11 12 |
  /------------\
  ```

通常来说 0 阶、1 阶和 2 阶的张量具有数学上的语义，更高阶的张量则是在标量、向量和矩阵上增加批量化维度。例如：

- “卷积”是矩阵和矩阵之间的运算，其输入输出数据的布局可以表示为 `[H,W]`；
- 但卷积通常会同时处理多个通道，于是增加了一个通道维度 C，输入输出数据的布局表示为 `[C, H, W]`，构成 3 阶张量；
- 如果要批量处理多个输入图片，则数据会再增加一个批量维度 N，输入输出布局表示为 `[N, C, H, W]`，构成 4 阶张量；

另外一个例子，基于 Transformer 的大语言模型有一种优化叫做 *kv cache*，*kv cache* 缓存张量的布局可以表示为 `[nblk, 2, nkvh, seq, dh]`，构成 5 阶张量。其中每个维度的语义分别是：

1. 多层堆叠的 Transformer 的层号；
2. 区分 k 缓存和 v 缓存；
3. 多头注意力的层号；
4. 注意力机制的序列长度；
5. 每个头的隐状态维度；

> 3 阶张量的示意图：
>
> ```plaintext
> shape: [z y x] = 2 x 3 x 4
> --------------------------
>   z
>  /
> *--x____________
> |  /          /|
> y / 13 14 15 16|
>  /----------/20|
>  |1  2  3  4|24|
>  |5  6  7  8| /
>  |9 10 11 12|/
>  /----------/
> ```

---

以上就是张量这种类型最基本的定义。但是在包括 pytorch 在内的许多复杂成熟的 AI 程序中，张量定义更复杂一些。

具体来说，张量的形状对于许多计算来说是具有语义的，这种语义会决定计算的方式。例如大家都学过矩阵乘法：

```plaintext
(M x N) <- (M x K) x (K x N)
```

这里出现的 3 个 2 阶张量它们各自的阶数和形状就决定了这个矩阵乘法的计算方式。但是对于基本的张量定义来说，形状和存储方式是一一对应的，所以为了得到需要的张量形状而对张量进行变换的时候就必须同时变换底层的数据表示。由于张量经常在很简单的形状下管理着海量数据，例如作为线性层权重出现的 4096x4096 的 f16 数组，对这样的矩阵转置意味着按每 2 字节一次，挪动 32MiB 字节，对于串行程序来说意味着 3355'4432 次寻址访问，这是非常低效的。

为了解决这个问题，需要把张量的逻辑形状和存储方式分离，变换时尽量只改变逻辑形状，只有在必要的时候才变换存储方式。我们采用的是一种经典的方式，就是为张量引入步长（`strides`）字段：

```rust
struct Tensor {
    data_type: DataType,
    shape: Vec<usize>,
    strides: Vec<isize>,
    data: Vec<u8>,
}
```

步长定义在张量的每一个维度上，表示这一个维度上第 i 个数据和第 i+1 个数据在底层存储上的距离。例如一个 C 二维数组：

```c
T arr[m][n];
```

它的步长就是 `(n, 1)`，因为在索引在长度为 `m` 的这个维度上每增加 1，就意味着数据的地址跳过了 `n` 个元素，因为矩阵的同一列的两个相邻元素之间隔着一行的长度——也就是列数——那么多个元素。而索引在长度为 `n` 的这个维度上每增加 1，地址也只增加了 1 个元素。这就是所谓行优先存储的矩阵。

所以对于 3x4 的二阶张量（矩阵），步长为 `[4,1]`：

```plaintext
shape:   [     y  x] = 3 x 4
strides: [(n) sy sx] = (12) 4 1
-------------------------------
\------------/    \----------/
| 1  2  3  4 |    |  O → +1  |
| 5  6  7  8 | -> |  ↓       |
| 9 10 11 12 |    | +4       |
/------------\    /----------\
```

对于 2x3x4 的三阶张量，步长为 `[12,4,1]`：

```plaintext
shape:   [     z  y  x] = 2 x 3 x 4
strides: [(n) sz sy sx] = (24) 12 4 1
-------------------------------------
  z
 /
*--x____________       ___________
|  /          /|      /         /|
y / 13 14 15 16|     /  +12    / |
 /----------/20|    /--/------/  |
 |1  2  3  4|24| -> | O → +1  |  /
 |5  6  7  8| /     | ↓       | /
 |9 10 11 12|/      |+4       |/
 /----------/       /---------/
```

如果按模型结构，某个矩阵运算需要转置矩阵，对于基本的张量表示来说真的需要挪动矩阵中所有的数据。而引入步长之后我只需要直接把形状和步长的 4 个数字交换一下就完成了：

|     | 原矩阵 | 转置矩阵
| :-: | :-: | :-:
| 形状 | M x N | N x M
| 步长 | N \| 1 | 1 \| N

如果认为这个修改算是优化加速的话，加速比就达到 (16MiB+2) / 4 = 419'4304.5 倍。

---

在 strides 之外，为了兼容 slice 和 split 变换，还需要一个 offset 字段描述序号为全零的元素与底层存储区域起始地址之间的距离。

> 另一种方案是直接修改底层存储区域的首地址。但是在张量持有底层存储所有权的情况下不能这么做。因此必须增加 offset 字段才能让张量变换对张量元信息变换封闭。

推导下列张量变换的表示方法留作作业：

1. transpose;
2. slice;
3. tile;
4. merge;
5. index;

## AI 程序的抽象

学习了张量之后，接下来开始介绍 AI 程序的抽象。

首先要解释的是为什么需要 AI 程序的抽象。换句话说，为什么我们不会像写一般的程序一样，使用一种通用编程语言来实现一个一般的程序，用这个程序进行神经网络计算？

用通用编程语言直接实现深度学习程序无疑本身是可行的。有一些著名的轻量级项目，比如 [llama2.c](https://github.com/karpathy/llama2.c) 就是这么做的。但是，具有实用价值的人工智能程序部署方案都是采用的 AI 编译器、推理引擎、编程框架这一类的方案。这种现象一方面当然有软件工程上解耦的必要性，但更主要的原因还是 AI 程序相对于通用程序具有范式上的转变。

具体来说，今天我们所说的 AI 程序，实际上已经不再指向所有符合机器学习定义，具有学习和泛化能力的程序，而是特指基于深度神经网络、操作大规模数据，并且几乎只跟数据产生交互的一种程序。AI 程序这样的特点使得开发者可以接受相比通用程序的开发者严格地多的限制，以换取开发的便利性。

AI 程序具有下述 3 个特点使得 AI 程序被设计成以下的模式：

1. 基于深度神经网络，因此 AI 程序具有很强的时序性或者层序性。换句话说，AI 程序中在层级结构上一般只有顺序执行，控制流较少（循环展开）。因此，AI 程序可以表示为有向无环图（DAG）；
2. 操作大规模数据，因此 AI 程序操作的基本数据类型不是单个变量，而是高维数组。在 AI 程序中，高维数组被称为张量（Tensor）；
3. 几乎无副作用，尤其是不需要与 IO 外设交互。因此 AI 程序可以完全由数据计算的函数调用组成，不需要 IO 模块。在 AI 程序中，函数调用被称为算子（Operator）；

至此，我们就推导出了 AI 程序最常见的一种表示形式，也是 InfiniTensor、RefactorGraph、ONNX 以及许多其他 AI 程序格式和 AI 编译器使用的表示形式——计算图。我们来看 [resnet](https://github.com/onnx/models/tree/main/validated/vision/classification/resnet) 的例子（<https://netron.app/>）。

> 学习过程中给，可以使用开源的 netron 应用实现可视化浏览 ONNX 文件，但是要开发接受 ONNX 输入的人工智能程序还是需要熟悉 API 的。

在 netron 可视化系统中，可以看到整个计算图实际上仅由 3 种元素组成。我们从上往下看：

- 最上面的 data 是模型的输入，同理，最下面还有模型的输出；
- 之后是箭头，可以看到这第一个箭头上还标记了一个形状。而且这个箭头是可以点的，我们点上去可以发现它实际上是一个张量；
  - 作为张量，它有数据类型、形状信息，可能还有数据。注意形状可以是未知的，因为除了输入形状，其他张量的形状都是被计算约束的，因此可以由输入张量的形状推导得到。实际上 AI 编译器前端最主要的功能就是形状推导，因此我们作业题也包括大量的形状推导题目；
  - 作为图上的边（Edge），它有源点和目标点信息。注意看 MaxPool 下面的箭头，这两个箭头指示的是同一个张量，指针指上去它们也是一起高亮的。查看内容发现这是一个单个源点两个目标点的边，这样的边表示它由源点计算产生，但是同时被多个目标点使用；
- 这种彩色的方框是图上的节点（Node），同时可以看到它是算子。无论作为算子还是作为节点，它具有类型（type）、模块（module）（这个模块实际上就是命名空间，为了避免类型重名用的）、名字（name）、属性（attribute）、输入（inputs）和输出（outputs）。注意，节点中实际上还隐含了一种信息，就是节点的一些输入实际上是常量，利于卷积算子的卷积核。这些常量本质上仍然是一些张量，只不过张量常量像全图输入一样不是由算子产生的。后面我把这种常量张量叫做图的内部边。可以把它们当作一种独特的东西，也可以当作一些特殊的张量；

如果往下翻，我们会发现整个图是由重复的一些结构组成的。实际上这就是一个循环结构，由于我们课程中会讲到的计算图中都是不体现控制流的，因此整个循环被展开到许多重复的结构。我这里展示的是最小版本的 resnet18，所以它层数很少。事实上有许多更大型的模型，例如大家都用过的 tiny llama 就是 22 层的 Transformer 模型，我们还有 50 层甚至 80 层的九格模型，可以想象如果对这样的模型可视化的话会得到特别巨大的一个图。

这些东西就组成了一个 resnet 模型。事实上，几乎任何 AI 程序都可以用输入输出、张量、算子这 3 种元素组成。由于这种可行性是我们刚刚根据那 3 个条件推导出来的，因此作为开发者可以信任这个结论。同时也要注意，一旦这 3 个条件不再满足，AI 编译器的这个设计也就立即会从合理简化转变为阻碍。同理，现在的现状是大语言模型经常需要一个独立的推理引擎，而不是接入一般的 AI 编译器，正是因为大语言模型引入了以 KV Cache 为首的许多针对生成类模型服务特点的优化手段，这些优化打破了 AI 程序顺序、无状态的假设。但是即使是加入了各种动态的优化手段，AI 程序的基础结构依然是深度神经网络，计算图的抽象在大部分情况下也仍然是有效的。

总结一下，计算图抽象是由拓扑结构、算子信息和张量信息组成的。那么有了计算图，可以操作的东西也就是这三大元素。对图上信息的操作就称作图变换。本次 AI 编译器中大家可能接触到的图变换操作包括张量推导、图优化和核函数选择和优化等。其中本节课会重点介绍张量推导，其他变换将在后续课程中讲解。

张量推导实际对应的是一般编程时的类型推导。例如 C++ 里可以使用 `auto` 关键字定义变量，变量的类型可以根据初始化表达式自动推导出来。作为编译器，类型推导功能是刚需，即使是前端不支持推导的语言比如 C 语言，在表达式嵌套时依然需要知道中间变量的类型才能完成编译。AI 编译器中操作的基本单元是张量，所以对应张量推导。

张量推导的逻辑是绑定在算子上的。一个算子就是一个函数，通过函数输入的类型确定输出的类型在通常情况下都是成立的。在 ONNX 中，推导的逻辑一般会出现在算子的文档上。对于 ONNX 这样不支持步长机制的 AI 程序表示中，要推导的就是输出张量的数据类型和形状。例如这个常见的单目运算符 [`Neg`](https://onnx.ai/onnx/operators/onnx__Neg.html)。根据文档，其输出张量的数据类型和形状都跟输入相同。

但是形状推导有可能也有复杂的逻辑。加减乘除这样的双目运算符，由于涉及形状广播，推导就十分复杂。以 [Add](https://onnx.ai/onnx/operators/onnx__Add.html) 为例，文档提到，其输出类型与输入相同，但是输出的形状是输入张量的双向广播。关于广播的具体操作在文档上有详细介绍，这里就略过了。

输出的张量信息还有可能取决于算子的属性（Attribute），例如变换数据类型的算子 [Cast](https://onnx.ai/onnx/operators/onnx__Cast.html)，其输出就是输入的形状配上 `to` 属性指定的类型。

更复杂的算子可能同时带有多种功能，例如 [Gather](https://onnx.ai/onnx/operators/onnx__Gather.html) 算子，其输入张量包括数据和序号，输出张量则是根据一套比较复杂的规则计算出来的。

最后，还有 2 类特殊算子，需要重点介绍。这两种算子的特殊之处在于它们涉及到元信息和数据之间的交互。一种是元信息变为数据，例如 [Shape](https://onnx.ai/onnx/operators/onnx__Shape.html) 算子；另一种是数据转到元信息，例如 [Reshape](https://onnx.ai/onnx/operators/onnx__Reshape.html) 算子。

第一种情况的代表——Shape 算子，其输出是一个向量，即一阶张量，张量的数据内容是输入张量的形状。对于可推导的计算图来说，这种算子的输出一定是常量，可以通过后续会介绍的常量折叠操作消除。

第二种情况的代表——Reshape 算子，输入是一个包含数据的张量和一个 int64 向量，表示输出的形状。由于形状推导时张量的数据是未知的，包含这种算子的计算图不可推导。对于这种算子的处理非常麻烦，本次课程会忽略它们。
