# InfiniLM 技术报告

大家好，今天由我来给大家介绍基于 Rust 的推理框架 InfiniLM。关于推理系统的通用知识在前几节课都介绍过了，本节课主要是基于这个实际可用系统的代码介绍一些具体的技术点。

## Rust 语言

由于本次训练营在之前的课程中大家可能都每太用到 Rust 语言，所以这里首先对 Rust 语言做个介绍。

![rusting](rusting.jpg)

如今 Rust 在网上热度相当之高，号称编程语言原神；但是相比传统的工业界使用的语言比如 C++、Java，渗透的领域又比较少。包括我在内很多 Rust 语言的拥护者经常被问到为什么要使用 Rust，Rust 和主流语言又有哪些差别？

| 维度         | Python                       | Java                           | C                            | C++                           | Rust                             |
| ------------ | ---------------------------- | ------------------------------ | ---------------------------- | ----------------------------- | -------------------------------- |
| **语法特点** | 动态类型，缩进控制作用域，GC | 静态类型，面向对象，GC         | 纯面向过程，指针操作，无 GC  | 支持面向对象，支持模板，无 GC | 所有权+借用检查，模式匹配，无 GC |
| **执行模式** | 虚拟机解释执行               | 编译到字节码然后虚拟机解释执行 | 编译到二进制                 | 编译到二进制                  | 编译到二进制                     |
| **应用领域** | AI/数据科学、Web、自动化脚本 | 企业后端、Android、大数据      | 操作系统、嵌入式、驱动开发   | 游戏引擎、高频交易、系统软件  | 互联网、系统软件                 |
| **构建工具** | pip + setuptools、Poetry     | Maven/Gradle/Ant               | Make、CMake、XMake ...       | Make、CMake、XMake ...        | Cargo（官方）                    |
| **库平台**   | PyPI（Python Package Index） | Maven Central                  | 无统一平台，多为源码分发     | 无统一平台，多为源码分发      | crates.io（官方，源码分发）      |

我这里列出了一个对比表格，大家可以看一下。总的来说，在语法和执行模式方面，Rust 和 C/C++ 一样，都是无 GC、无运行时、编译到二进制执行的语言，所以这三个语言可用于系统编程，在交互性方面也比较方便。除此之外，Rust 由于有官方的构建工具和库分发平台，查找和使用第三方库的难度相比 C/C++ 来说低很多。虽然目前整体上来说 Rust 在 AI 系统领域高质量的库还不太多，但是假以时日的话我相信 Rust 会越来越好用的。

由于在前面课程的学习中大家应该也没太接触到 Rust，本次课程介绍技术的过程中我也不会特别依赖 Rust 语法。这里只用一个小例子来介绍 Rust 和 C/C++ 在软件建模上的差别。

在多线程的并行编程中，锁是一种非常常用的抽象。以互斥锁为例，

C++ 的[互斥锁](https://zh.cppreference.com/w/cpp/thread/mutex)是这个样子：

```c++
std::mutex mutex; // 锁对象
{
    std::lock_guard<std::mutex> guard(mutex); // 控制上锁区域的 RAII 对象
    ...
}
```

Rust 的[互斥锁](https://doc.rust-lang.org/std/sync/struct.Mutex.html)是这个样子：

```rust
let data: Mutex<i32> = Mutex::new(0); // 锁对象，隐去了常与 Mutex 配合的 Arc
{
    let mut data = data.lock().unwrap(); // 控制上锁区域的 RAII 对象
    ...
}
```

抛去语法上的差异，这两份代码其实是相当接近的。但是这两份代码也展示了 C++ 和 Rust 在对互斥锁的理解上根本性的不同，即：C++ 的 `std::mutex` 锁住的是时序；Rust 的 `Mutex` 锁住的是对象。

直观理解这两种抽象的差别，我们可以补全要使用互斥锁保护的对象：

```c++
std::mutex mutex;
int counter = 0;
{
    // 程序员需要保证：锁 mutex 真的保护了 counter 吗？
    std::lock_guard<std::mutex> guard(mutex);
    counter += 42;
}
```

```rust
let counter: Mutex<i32> = Mutex::new(0);
{
    // 锁和数据被捆成单个变量 counter
    let mut num = counter.lock().unwrap();
    *num += 42; // 不锁定，编译器不让你碰 num
}
```

通过这个例子希望大家对于 Rust 的安全性建立一个直观的印象。就像这个例子一样，Rust 利用语法层面的生命周期和借用检查，巧妙地通过抽象将许多原本只能在运行期测试的错误纳入了编译期可检查的范围。

## InfiniLM 的结构

现在正式进入对 InfiniLM 项目的介绍。InfiniLM 作为一个实际应用的系统，具有多个逻辑组件，能完成的功能也比较复杂。

目前 InfiniLM 的仓库有 3 个主要的分支：

1. version1
2. main
3. llama.cu

这 3 个分支的代码架构都非常不同，我们今天会基于最新的 llama.cu 这个分支介绍最新的功能和结构。llama.cu 这个分支目前支持且仅支持在英伟达显卡和 cuda 兼容的国产硬件上推理。可以生成文本、对话、还可以打开 OpenAI 兼容的 web API。对话的功能相对来说好操作，也比较有代表性。只需要用这个命令就可以启动了：

```shell
cargo chat `model`
```

### InfiniLM 的逻辑组件

llama.cu 这个分支里面，InfiniLM 可以大致分为 4 个逻辑组件，分别是：

1. 模型构建工具：使用预定义的的神经网络结构递归搭建模型结构；
2. 图表示和图优化：将 NN 搭建的模型结构转化为计算图，并变换和优化计算图；
3. 运行时和调度器：加载变换后的计算图，调度计算逻辑；
4. 应用程序：处理参数、用户请求，跟引擎核心交互；

其中模型结构、图表示和硬件无关的预定义优化在一个独立的项目 InfiniNN 中，运行时和调度器在 InfiniLM 的 llama.cu crate 里，应用程序在 xtask crate 里。我会从这些组件里面挑出一些比较有特色的技术点给大家介绍。

## 技术点

### 1. 模型构建

在模型构建的部分，为了在具有结构共性的大模型之间复用结构的定义，InfiniNN 采取的是类似 PyTorch 的神经网络组件的抽象。InfiniNN 中预定义了很多常用的结构组件。结构组件需要实现一个 `NuralNetwork` 特质。

```rust
pub trait NuralNetwork<T>: Sized {
    fn launch(
        self,
        inputs: impl IntoIterator<Item = Tensor<T>>,
        ctx: Context<T>,
    ) -> Result<(Context<T>, Vec<Tensor<T>>), NNError>;
}
```

这个特质实际上定义了这个神经网络的传播过程，所以它的参数是神经网络本身、输入的张量和一个用于与环境交互的上下文结构，返回操作过的上下文以及输出的张量。在传播过程中，神经网络会将张量传递给子网络或者通过算子来改变张量的元信息和数据。

例如，线性层是这样定义的：

> 示例中为了保持代码的简单性，去除了可见性等语法结构。

```rust
struct Linear<T> {
    dt: DigitLayout,   // 线性层权重的数据类型
    shape: [usize; 2], // 线性层的核心操作是矩阵乘，这是权重矩阵的形状
    weight: T,         // 权重矩阵
    bias: Option<(DigitLayout, T)>, // 线性层偏置，可能有
    allow_residual: bool, // 线性层可以融合残差连接
}

impl<T> NuralNetwork<T> for Linear<T> {
    fn launch(
        self,
        inputs: impl IntoIterator<Item = Tensor<T>>,
        mut ctx: Context<T>,
    ) -> Result<(Context<T>, Vec<Tensor<T>>), NNError> {
        let Self {
            dt,
            shape,
            weight,
            bias,
            allow_residual,
        } = self;
        let [r, c] = shape;

        // 从外部加载权重矩阵
        let w = ctx.load_external("weight", dt, [r.into(), c.into()], weight);

        let mut inputs = inputs.into_iter();

        // 提取第一个输入张量
        let x = inputs.next().unwrap();
        let outputs = match inputs.next() {
            // 融合残差连接的情况
            Some(residual) if allow_residual => match bias {
                Some((dt, bias)) => {
                    // 有偏置，从外部加载偏置矩阵
                    let b = ctx.load_external("bias", dt, [r.into()], bias);
                    // 调用线性算子
                    ctx.call("", "linear", Some(true.into()), [x, residual, w, b])
                }
                None => {
                    // 调用线性算子
                    ctx.call("", "linear", Some(true.into()), [x, residual, w])
                }
            },
            // 不融合残差连接的情况
            _ => match bias {
                Some((dt, bias)) => {
                    // 有偏置，从外部加载偏置矩阵
                    let b = ctx.load_external("bias", dt, [r.into()], bias);
                    // 调用线性算子
                    ctx.call("", "linear", Some(false.into()), [x, w, b])
                }
                None => {
                    // 调用线性算子
                    ctx.call("", "linear", Some(false.into()), [x, w])
                }
            },
        };

        Ok((ctx, outputs?))
    }
}
```

看一个比较复杂的例子，自注意力层是这样的：

```rust
// 自注意力层是由 qkvo 4 个线性层和注意力运算组成的
// 其中 qkv 线性层可以融合，这个融合需要改变权重存储方式，所以必须离线进行
// 在线性层和注意力计算之间可以插入归一化和旋转位置编码
struct Attention<T> {
    nh: usize,
    nkvh: usize,
    qkv: Linear<T>,
    q_norm: Option<Normalization<T>>,
    k_norm: Option<Normalization<T>>,
    rope: Option<RoPE<T>>,
    output: Linear<T>,
}

// 旋转位置编码有自己的参数和权重
struct RoPE<T> {
    multimodal: bool,
    nctx: usize,
    sin: T,
    cos: T,
}

impl<T> NuralNetwork<T> for Attention<T> {
    fn launch(
        self,
        inputs: impl IntoIterator<Item = Tensor<T>>,
        mut ctx: Context<T>,
    ) -> Result<(Context<T>, Vec<Tensor<T>>), NNError> {
        destruct!([x, pos, residual] = inputs);

        let Self {
            nh,
            nkvh,
            qkv,
            q_norm,
            k_norm,
            rope,
            output,
        } = self;
        // 陷入 qkv 线性层，“自”注意力体现在 qkv 的融合
        destruct!([x] = ctx.trap("attn-qkv", qkv, [x])?);
        dims!([_, dqkv] = x);
        let dh = dqkv.clone() / (nh + nkvh + nkvh);

        // 调用 split 算子分开 qkv 输出，这是算子调用的简单写法，跟 `ctx.call` 功能等价
        destruct!([q, k, v] = x.split("split-qkv", 1, [nh.into(), nkvh.into(), nkvh.into()])?);

        // 如果 q 需要归一化
        let q = match q_norm {
            Some(norm) => {
                let q = q.tile("", 1, [nh.into(), dh.clone()])?;
                destruct!([q] = ctx.trap("attn-q-norm", norm, [q])?);
                q.merge("", 1, 2)?
            }
            None => q,
        };
        // 如果 k 需要归一化
        let k = match k_norm {
            Some(norm) => {
                let k = k.tile("", 1, [nkvh.into(), dh.clone()])?;
                destruct!([k] = ctx.trap("attn-k-norm", norm, [k])?);
                k.merge("", 1, 2)?
            }
            None => k,
        };
        // 如果 qk 需要旋转位置编码
        let [q, k] = match rope {
            Some(RoPE {
                multimodal,
                nctx,
                sin,
                cos,
            }) => {
                let shape = [nctx.into(), dh.clone() / 2];
                let sin = ctx.load_external("rope.sin", types::F32, shape.clone(), sin);
                let cos = ctx.load_external("rope.cos", types::F32, shape, cos);

                let op = if multimodal { "mrope" } else { "rope" };
                destruct!(
                    [q_] = ctx.call(
                        "attn-q-rope",
                        op,
                        None,
                        [q, pos.clone(), sin.clone(), cos.clone()]
                    )?
                );
                destruct!([k_] = ctx.call("attn-k-rope", op, None, [k, pos, sin, cos])?);
                [q_, k_]
            }
            None => [q, k],
        };

        // 执行注意力计算
        destruct!([o] = ctx.call("", "attention", Some(dh.into()), [q, k, v,])?);
        // 陷入 output 线性层
        let outputs = ctx.trap("attn-output", output, [o, residual]);

        Ok((ctx, outputs?))
    }
}
```

可以看到，在自注意力结构中复用了线性层和归一化层的定义。这种递归的定义方式减少了重复代码。

### 2. 稠密的计算图表示

有了递归搭建的神经网络结构之后，只需要执行一遍神经网络的 `launch` 方法就可以把神经网络执行中用到的张量和算子都记录下来，生成一张计算图。

这里 InfiniLM 用到一种跟 InfiniTensor 很不一样的计算图表示方法。在 InfiniTensor 以及其他一些 AI 编译器中，计算图是使用大量指针组织起来的一个在存储上十分稀疏的结构。这是 InfiniTensor 中张量定义的片段，可以看到张量会关联产生和消费自己的算子：

```c++
protected:
    int dim;
    DataType dtype;
    vector<WRef<OperatorObj>> targets;
    WRef<OperatorObj> source;
    Blob data;

private:
    Shape shape;
```

这是算子定义的片段，可以看到算子也会关联自己的输入输出张量、还有自己的前驱后继算子。

```c++
protected:
    OpType type;
    TensorVec inputs;
    TensorVec outputs;
    vector<WRef<OperatorObj>> predecessors;
    vector<WRef<OperatorObj>> successors;
```

对于一个完整的计算图，可能涉及到数百个算子、数千个张量，这些对象会随着构图和图变换，悬浮在整个地址空间上任意的位置，相当于是一个图的链表。访问尤其是遍历这样的数据结构会有极大的性能损失。另外，在图优化的不同阶段需要不同的节点和边，这种图结构也不方便剥离节点和边的具体定义。为了简化多层的图结构定义，优化遍历性能，InfiniLM 中采用了一种特殊的稠密图数据结构。代码是这样的：

```rust
struct GraphTopo {
    n_inputs: usize,
    n_outputs: usize,
    connections: Box<[usize]>,
    nodes: Box<[TopoNode]>,
}

struct TopoNode {
    n_local: usize,
    n_inputs: usize,
    n_outputs: usize,
}
```

这种数据结构用序号和数量描述图的拓扑连接关系，与节点和边的具体类型无关。因此存储高度压缩，具有很好的访问和遍历性能。

具体来说，`GraphTopo` 结构体的 4 个字段分别是图的全局输入边数量、全局输出边数量、连接关系序号和节点信息。节点信息中保存的是节点的输入边数量、输出边数量和所谓局部边数量，这个局部边可以理解为常量，即不是由节点生成、也不是全图输入边，但被这个节点消费的边。

这些数字为什么就能表示图的连接关系呢？实际上这些数据在逻辑上可以映射到一个节点表和一个边表。节点表就是 `GraphTopo` 结构体中的节点：

| 节点序号 | 局部边 | 输入边 | 输出边 |
| :------: | :----: | :----: | :----: |
| 0        | l0     | i0     | o0     |
| 1        | l1     | i1     | o1     |
| ...      | ...    | ...    | ...    |
| n        | ln     | in     | on     |

但是我们实际希望的是连接关系，也就不能是数量，而应该是一系列序号。但是每个节点的输入输出数量是不同的，也不好设置一个最大值，如果要直接在节点上存储序号的话就不得不使用 `Vec` 之类的变长容器，这会导致拓扑存储进一步碎片化。因此这里对数据结构做了一个转化。`GraphTopo` 结构体中并没有直接存储一个边表。这个边表实际上是由节点表隐式映射出来的。我们根据边的所有权关系，从节点表信息中虚拟出边的顺序和序号。因为每个边只能有 0 个或 1 个源节点，因此我们让边挂载在产生它或者第一次使用它的节点名下，也就能构造这样一个边表：

| 边序号        | 类型     | 源节点 | 序号最小的目标节点 |
| :------------:| :------: | :----: | :----------------: |
| 0             | 全局输入 | -      | ?                  |
| 1             | 全局输入 | -      | ?                  |
| ...           | ...      | ...    | ...                |
| n_inputs-1    | 全局输入 | -      | ?                  |
| n_inputs      | 局部边   | -      | 0                  |
| n_inputs+1    | 局部边   | -      | 0                  |
| ...           | ...      | ...    | ...                |
| n_inputs+l0-1 | 局部边   | -      | 0                  |
| n_inputs+l0   | 一般     | 0      | ?                  |
| n_inputs+l0+1 | 一般     | 0      | ?                  |
| ...           | ...      | ...    | ...                |

有个这个边表，就实现了对每个边的唯一定位。在边序号的基础之上可以定义节点的输入边，也就是 `GraphTopo` 结构体中的 `connections` 字段：

| :-------------:| :------: | :------: |
| 连接关系序号   | 类型     | 目标节点 |
| 0              | 全局输出 | -        |
| 1              | 全局输出 | -        |
| ...            | ...      | ...      |
| n_outputs-1    | 全局输出 | -        |
| n_outputs      | 节点输入 | 0        |
| n_outputs+1    | 节点输入 | 0        |
| ...            | ...      | ...      |
| n_outputs+i0-1 | 节点输入 | 0        |
| ...            | ...      | ...      |

经过映射之后，节点表上的数量转化为节点表和连接表上的区间，遍历区间可获得序号：

| 节点序号 | 局部边           | 输入边            | 输出边           |
| :------: | :--------------: | :---------------: | :--------------: |
| 0        | 节点表里的 l0 项 | 连接表里的 i0 项  | 节点表里的 o0 项 |
| 1        | 节点表里的 l1 项 | 连接表里的 i1 项  | 节点表里的 o1 项 |
| ...      | ...              | ...               | ...              |
| n        | 节点表里的 ln 项 | 连接表里的 in 项  | 节点表里的 on 项 |

在拓扑的基础之上，再补充两个简单的数组结构，就可以把序号映射到具体信息：

```rust
struct Graph<N, E> {
    topo: GraphTopo,
    nodes: Box<[N]>,
    edges: Box<[E]>,
}
```

这种拓扑和信息严格解耦的数据结构，可以很方便地在不修改拓扑的情况下替换节点和边的类型。这种特性非常有利于多级图表示。

### 3. 元信息变换和存储区生命周期分析

接下来介绍 InfiniLM 里做的一个比较有特色的图优化方法。

在 AI 编译器的课程中介绍过，张量的关键信息包括数据类型、形状和存储布局，一些张量变换可以表示为对存储布局的变换，而不需要实际操作数据。在神经网络构建的过程中，张量因为没有实际加载，也就不存在存储布局，这些张量变换和针对数据的计算一样，会视作一个算子标注到计算图上。存储布局信息是在图变换过程中补充到图上的。

在图变换的一个阶段，会为所有张量生成存储布局信息。最初，认为所有张量都是连续、稠密、行优先或者说大端的存储布局。也就是说，在形状中越靠后的维度越连续，例如：

| 维度序号 | 0  | 1  | 2 | 3 |
| :------: |:--:|:--:|:-:|:-:|
| 形状     | 2  | 3  | 4 | 5 |
| 步长     | 60 | 20 | 5 | 1 |

接下来，搜索图中所有可以转换为张量变换的算子（节点），改变其输出张量的布局，标记其复用输入张量的存储区域，最后将算子标记为空。

![元信息变换](tensor-meta.png)

这样的变换可以多次发生，连续的元信息变换会全部变为空。

### 3. GGuf 模型文件

在运行时部分，有几个比较有意思的东西可以介绍。

首先，不同于 HuggingFace 定义的 config.json/model.bin/model.safetensors，InfiniLM 从 GGuf 格式加载模型。
HuggingFace 的经典方式将模型存储在多个文件中，且把模型配置信息写在人类可读写的 json 文件中，这导致模型的定义方式非常混乱。每个模型都有自己的一套配置方式，难以复用；多个文件的版本管理也比较繁琐。

GGuf 格式是 llama.cpp 项目定义的一种二进制存储格式。如图所示，GGuf 格式是一种 All In One 的架构，将模型的所有信息都存储在单个文件中，管理起来十分简单。并且其元信息是以二进制形式存储，可以直接通过 memory mapping 映射到内存，整段拷贝和解析，读取性能相对于大量使用文本的 json 格式更高。

![gguf](https://hugging-face.cn/datasets/huggingface/documentation-images/resolve/main/hub/gguf-spec.png)

在 InfiniLM 决定采用 GGuf 格式时，Rust 社区还没有好用的 GGuf 操作库，因此我们自己开发了一个并发布到了 crates.io 平台。使用我们开发的 [ggus](https://crates.io/crates/ggus) 库加载 GGuf 文件非常容易，而且性能极高：

```rust
let file = File::open(file_path)?;       // 打开文件
let data = unsafe { Mmap::map(&file) }?; // 映射文件
let gguf = GGuf::new(&data)?;            // 解析 GGuf
```

### 4. 异步模型加载

### 5. 虚存管理

### 6. 异步调度
